
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}
%\linespread{2.5}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}

\usepackage{url}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\newcommand{\BigO}[1]{$\mathcal{O}{(#1)}$}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Hyperplane Elimination for Quickly Enumerating Local Optima}

% a short form should be given in case it is too long for the running head
%\titlerunning{Lecture Notes in Computer Science: Authors' Instructions}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
%\author{Brian W.~Goldman\and William F. Punch}
\author{Anonymous Author\and Anonymous Author}

%
%\authorrunning{Lecture Notes in Computer Science: Authors' Instructions}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
%\institute{BEACON Center for the Study of Evolution in Action,\\
%Michigan State University, U.S.A.\\
%brianwgoldman@acm.org, punch@msu.edu}
\institute{Department\\ Organization\\ Email}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
TODO

%The abstract should summarize the contents of the paper and should
%contain at least 70 and at most 150 words. It should be written using the
%\emph{abstract} environment.
\keywords{Landscape Understanding, Gray-Box, MK Landscapes}
\end{abstract}


\section{Introduction}
The ruggedness and high dimensionality of most interesting landscapes makes them challenging
to visualize or otherwise analyze. However, doing so can be helpful in
quantifying the difficulty of a problem, and how to best design algorithms to deal
with those difficulties. 
Similarly, knowing which characteristics favor
a particular
algorithm can help researchers choose the algorithm
most likely to perform well on their problem.

One avenue for performing this problem understanding is to
examine the frequency and distribution of local optima~\cite{boese:1994:bigvalley},
and how operators allow search to transition between these
optima~\cite{tomassini:2008:nknetworks,verel:2011:nknetworks,ochoa:2015:crossovernetworks}.
However, finding all of the local optima is prohibitively time consuming
for even small problems, with many studies limited to 18-bit
problems~\cite{tomassini:2008:nknetworks,verel:2011:nknetworks}.
By leveraging recent advancements in Gray-Box optimization which allow for
constant time local search~\cite{chicano:2014:ball}, this limit was raised
to 30-bit problems~\cite{ochoa:2015:crossovernetworks}.

Here we will introduce a method for finding all local optima of problems
with up to 80 bits. The cornerstone of this method is the identification
of hyperplanes which cannot contain any local optima, allowing large
portions of the search space to be skipped during enumeration.



\section{Mk Landscapes and Gray-Box Optimization}
An Mk Landscape is any function which can be expressed in the following form:
\begin{equation}
  f(x) = \sum_{i=1}^{M} f_i(mask(x, s_i))
  \label{eq-mk}
\end{equation}
In this equation there are $M=$\BigO{N} subfunctions $f_i$, which act
on some subset $s_i$ of variables in $x$. Each subfunction performs the mapping
$f_i : \mathbb{B}^{k_i}\rightarrow \mathbb{R}$ where $k_i \leq k$ and $k$ is
a constant with respect to $N$. The $mask$ function returns the values of $x$ for
the variables in $s_i$.

A critical aspect of this problem class is that it can be optimized using Gray-Box
techniques, which can allow for highly efficient local~\cite{whitley:2012:constant}
and global~\cite{goldman:2015:GBO,tintos:2015:partitioncross} search. These algorithms
exploit the known and limited non-linearity between variables expressed by all $s_i$,
as well as perform partial solution evaluation using all $f_i$.

This formulation can represent many problems of real-world interest, as well as
the most commonly used combinatorial benchmark problems. In this work we shall
examine 5 problems in particular: Concatenated Traps, Adjacent and Random NKq Landscapes,
Ising Spin Glasses, and MAX-kSAT.

The Concatenated Trap problem~\cite{deb:1992:trap} is a composition of $k$-order deceptive
separable subfunctions. In Mk Landscape terms, $M=N/k$ such that $\forall_i |s_i| = k$ and
$\forall_{i \neq j} s_i \cap s_j = \emptyset$. Each $f_i$ applies an identical function based
on the number of variables set to 1:
\begin{equation}
   trap(t) = \left\{
     \begin{array}{rl}
       k-1-t, &  t<k\\
       k,   &  t = k
     \end{array}
   \right.
  \label{eq-trap}
\end{equation}
While this problem is still in common use~\cite{hsu:2015:dsmgaII,inoue:2015:adaptivep3},
most advanced methods can solve it trivially~\cite{goldman:2012:ltga} and when expressed
as an Mk Landscape can be solved exactly in \BigO{N} time~\cite{whitley:2015:mk}. However,
we include it here as its simple structure allows for straight forward algorithm analysis.

NKq landscapes specify a class of randomly generated problem instances using 3 paramters:
(1) the number of problem variables $N$ (2) the amount of variable epistasis $K$ where $K=k-1$
and (3) the number of unique fitness values $q$. From these parameters a landscape is generated
by creating $M=N$ subfunctions $f_i$, where $f_i$ uses variable $i$ and $K$ others to look up
a fitness value in the range $[0..q-1]$ from a randomly generated table. This structure
makes NKq landscapes a natural fit for Mk landscapes (sum of bounded subfunctions)
and are the most studied problem class for Gray-Box
optimization~\cite{whitley:2012:constant,goldman:2015:GBO,tintos:2015:partitioncross,ochoa:2015:crossovernetworks,whitley:2015:mk}.
Here we choose $k=3$ with $q=2^{K+1}=2^{k}=8$.

We use two common variants of NKq in our experiments which specify how the $K$ additional
variables in each subfunction are chosen: Adjacent and Random. In Adjacent NKq, $f_i$ depends
on variables $[i..(i+k)\%N]$. In Random NKq, $f_i$ depends on variables $i$ and a random
set of $K$ unique variables which does not include $i$. While Random NKq is NP-Hard,
the structure of Adjacent NKq allows for a polynomial time dynamic programming solution~\cite{wright:2000:solvingnk}.
Adjacent NKq is therefore often used to understand a search algorithm's ability to reach the known global
optimum.

Ising Spin Glasses are a type of MAX-CUT problem derived from statistical physics.
Each atom in the glass (vertex) can be assigned a spin, with the goal being to
find the set of assignments which minimize the energy between nearby atoms (edges).
Similar to Adjacent NKq, the $2D\pm J$ subset of Ising Spin Glasses can be polynomially
solved~\cite{saul:1994:spinglass}\footnote{\url{http://www.informatik.uni-koeln.de/spinglass/}}.
In this subset, the graph is defined as a square two-dimensional torus with each edge
weight chosen from ${-1, 1}$. Each vertex is assigned a spin from ${-1, 1}$ with
the energy in the glass equal to
\begin{equation}
\sum_{e_{ij} \in E} x_ie_{ij}x_j
  \label{eq-ising}
\end{equation}
where $e_{ij}$ is the edge connecting vertex $i$ to vertex $j$. In Mk landscape terms
this type of spin glass has $M=2N$ and $k=2$.

Our final examined problem is randomly generated maximum satisfiability or MAX-kSAT.
This version of the canonical NP-Complete boolean satisfiability problem is formulated
as the maximization of $M=4.27N$ clauses, each containing exactly $k=3$ unique literals.
A clause is satisfied if any of its literals match how a solution's variables are set.

\section{Gray-Box Enumeration}
When considered as a black box, the process of finding all local optima
in a landscape requires \BigO{N2^N} time. This is because evaluating
a solution requires $N$ time, and all $2^N$ solutions in the landscape
must be evaluated once. Finding the local
optima then requires each solution to compare its fitness with each
of its neighbors, which when properly cached takes \BigO{N} per solution.
Extending this method to look for solutions which cannot be improved by
flipping $r$ or fewer bits requires \BigO{N^r2^N} time. However,
by exploiting Gray-Box optimization methods previous work~\cite{ochoa:2015:crossovernetworks}
was able to find all $r$-bit local optima of Mk Landscapes in \BigO{2^N} for any small,
fixed value of $r$.

The first major result in Gray-Box optimization was the proof that the list
of fitness improving moves during local search can be kept updated in \BigO{1}
time~\cite{whitley:2012:constant}. Each time a modification is made to a solution
only modifications which share a non-linear relationship with the changes can
go from non-improving to improving and vice versa. In Gray-Box optimization
the non-linearity between variables is known. Furthermore the number of
non-linear relationships is bounded above by \BigO{Mk} as each of the $M$
subfunctions can cause a non-linear relationship between each of its $k$
variables. As $M=$\BigO{N} and $k$ is constant, the amortized number of affected
flips per variable is \BigO{1}.

This limited and known non-linearity can be used to allow search to find only $r$-bit
local optima. Consider a variable interaction graph, such that each variable maps
to a vertex in the graph and each edge represents a non-linear relationship between
variables. TODO

TODO Explain increased radius size~\cite{chicano:2014:ball}


\section{Hyperplane Elimination}

\section{Improved Enumeration Ordering}

% Proof sketch
Moves can have their dependencies fixed in some permutation $O$ of all possible moves,
such that $O_0$ is the first move to have its dependencies fixed.
$D_O(i, m)$ is a function which returns how many unfixed dependencies move $m$ has after all
$i$ moves in $O$ before $i$ have been fixed. A greedy solution to this problem works such
that $O_i$ is set to be the move which minimizes $D_O(i, m)$.

All non-greedy solutions must have some $i$ such that $D_O(i, O_i) > D_O(i, O_{i+1})$.
This is because $D_O(i, m)$ can only decrease as $i$ increases, meaning that if some $m$
has a lower value at $i$ than $O_i$, some index between $i$ and when $m$ appears in $O$
this property must hold. Finally, all non-greedy solutions must have some $\hat{i}$
which is the maximum $i$ value for which $D_O(i, O_i) > D_O(i, O_{i+1})$ is true. 

Consider the optimal ordering $O^\star$ which is not greedy.
These criteria mean that $D_{O^\star}(\hat{i}, O_{\hat{i}}) > D_{O^\star}(\hat{i}, O_{\hat{i}+1})$.
Swapping $O_{\hat{i}}$ and $O_{\hat{i}+1}$ cannot change the minimum dependency of any other moves.
Performing the swap will improve the minimum dependency of $O_{\hat{i}+1}$ more than it hurts
$O_{\hat{i}}$. Therefore, performing this swap will make $O^\star$ even better, implying it was
not optimal before. Therefore there cannot exist an optimal ordering which is not greedy.

\section{Complexity Classes for Simple Landscapes}

\subsection{OneMax}
In OneMax, each bin contains exactly 1 move. All variables in the current solution
which disagree with the global optimum will be fitness improving moves.
Initially $index$ is set to $N-1$ and is decreased by at most 1 in any step.
Before finding the global optimum no carry operations can occur, meaning the
cost of each iteration is equal to the amount by which $index$ is decreased.
Therefore, Algorithm~\ref{alg-enumerate} requires \BigO{N} time to reach
the global optimum.

After finding the global optimum of OneMax, $index$ is set to 0. One iteration
is then spent adding a 1 to $index$ 0, which in the worst case requires \BigO{N}
carry operations. For all future iterations $index$ is a fitness improving move
and is currently set to 1. In these iterations at least 1 carry operation must
occur, and $index$ cannot be decreased. Iteration ends when $index$ exceeds $N$
which requires at most $N$ carry operations. Therefore, Algorithm~\ref{alg-enumerate}
requires \BigO{N} time to reach termination after finding the global optimum. Combined
with initialization and the time to find the global optimum, the total complexity is \BigO{N}.

\subsection{$k$-Bound Separable Problem}
Consider any problem which is composed of non-overlapping
subfunctions, each using using $k$ or fewer bits. Regardless
of how they are initially ordered, reordering will ensure that
variables of a function are always consecutively placed in enumeration.
Consider a numbering of $f_i$ such that if $i < j$ then all of $f_i$'s
variables appear before $f_j$'s in enumeration ordering.

While any $f_i$ contains a fitness improving move, Algorithm~\ref{alg-enumerate}
will find the one with the highest $i$ and enumerate it until it no longer
contains an improving move. In total this enumeration requires less than $2^k$
steps. Each $f_i$ is considered sequentially, meaning the first local optimum
will be found in $2^kN/k$ time.

Once the first local optimum is found, Algorithm~\ref{alg-enumerate} proceeds
by finding all local optimum in larger and larger hyperplanes. Initially $f_0$
is enumerated to find all local optima in the hyperplane with all $f_i, i>0$
fixed. This process ends when a carry operation modifies a bit of $f_1$. At
this point $f_1$ is enumerated, such that each time $f_1$ contains no improving moves
$f_0$ is enumerated again. This finds all local optima in the hyperplane with $f_i, i>1$
fixed. We represent the time required to find all local optima in the hyperplane
with $f_j, j>i$ as $T(i)$. $T(0)=2^k$ as all values of $f_0$ must be enumerated.
$T(i) = |l_i|*T(i-1)+2^k$ where $|l|$ is the number of ways $f_i$ can be set such
that it contains no fitness improving moves. Each $l_i$ exposes a new hyperplane
which can contain local optima, which causes the recursive call to $T(i-1)$.
Assuming for simplicity that all subfunctions use exactly $k$ bits,
the time required to find all local optima is $T(N/k-1)$.
Assuming that $\forall_i |l_i|=c$, $T(N/k-1)=\sum_0^{N/k-1}2^kc^i<2^kc^{N/k}$.
The number of local optima in the landscape is $c^{N/k}$ meaning that Algorithm~\ref{alg-enumerate}
is within a constant factor of $2^k$ of optimal.

Concatenated Traps is a commonly used $k$-bound separable problem
with $c=2$.  Algorithm~\ref{alg-enumerate} requires $2^k2^{N/k}$ time
to find all $2^{N/k}$ local optima in this problem.

\section{Timing Comparisons}
\subsection{Increasing problem size}
Consider including boxplots of largest size all are successful.

\subsection{The Effect of Radius}

\section{Examining Local Optima}

TODO Consider discussing neutral networks

\section{Conclusions}

\subsubsection*{Acknowledgments.} TODO Acknowledge BEACON.

\bibliographystyle{splncs03}
\bibliography{../main}

\end{document}
