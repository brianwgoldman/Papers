\documentclass[]{article}
\usepackage{hyperref}

\begin{document}

We would like to first take the opportunity to thank our reviewers for their thorough and insightful comments. It is for this reason, the review process and diligent reviewers, that papers are of such high quality in Evolutionary Computation.

We believe that the comments can be broken into three classes:
\begin{itemize}
\item Errors in the paper that have been corrected.
\item Minor fixes/modifications that we have addressed.
\item Items of disagreement
\end{itemize}

\section{Errors}
\begin{quote}
 When testing the LTGA outside P3 however, the code is still recomputing the entropies to update the mutual information matrix. This is highly inefficient, the original LTGA only has to recompute the mutual information matrix once in each generation, not whenever a better solution is found. The experiments with LTGA should be run with the original LTGA implementation, not the modified P3 version. The code can be found at \url{http://homepages.cwi.nl/~bosman/source_code.php}
\end{quote}

This was indeed an error on our part. Unfortunately the source code was not made available to us, despite a request,
and the code was not released to the public until after the paper was submitted. The fix,
a two line modification, does not affect any of the evaluation counts but does indeed modify our
wall clock time experiments. Subsequently, after moving the entropy calculation,
we were able to reach timing parity with Dr. Bosman's implementation. We have rerun all of our timing results
and updated our findings accordingly. In general we find that for single instance problems,
optimally tuned LTGA %(where we of course do not count the evaluations or timing necessary to find the optimal population size)
finds the global optimum using less seconds than P3. However, on randomly generated problem classes,
P3's reduction in evaluations overcomes its increased CPU usage per evaluation.
We have updated our figures and discussion to reflect these changes.

\section{Fixes}
\subsection{Reviewer 1}
\begin{itemize}

\item
\begin{quote}
P3 seems to be very noisy in its results: the paper only reports median results after 100 runs. More information about variance and outliers (e.g. through boxplots) should be shown for at least one hard problem, for instance spin glasses. 
\end{quote}

In the original submission Section 9.1 explicitly discussed the variance in outcomes,
and Figure 10 showed the upper and lower quartiles (just as boxplots do) for all 7 problems.
We have, however, replaced Figure 10 (Now Figure 11) with a boxplot to show outliers.

\item
\begin{quote}
P3 stores all improving solutions ever found in a huge hash table. It should be reported how many solutions are typically stored and again what is the variance in these numbers. We have found that this varies a lot. 
\end{quote}

While the original work did not explicitly report the aggregate value,
you can estimate it in the original submission using the area under each curve in Figure 13a.
We have added a new figure (Figure 10, parts A and B) to more clearly show total number of solutions stored.

\item
\begin{quote}
My main point would be that their paper shows that the EA community should rethink its population management scheme and not blindly follow the fixed population size generational scheme.
\end{quote}

We agree, though our overall point is more general in that we are trying to get the community to rethink
\textbf{all} parameter settings. In the original submission Section 9, Section 10.2,
Figure 11, and Figure 13a explicitly discuss the benefits we believe P3 achieves due
to side stepping the fixed size generational scheme.
We also had previously mentioned it in the third paragraph of the conclusion.
Nonetheless, we have now added some additional references and discussion to the introduction to elevate its importance.

\item
\begin{quote}
Only strictly better solutions are excepted: this is a big problem for LTGA with (at least) MAXSAT
\end{quote}

We agree that it is important for neutral drift to occur during mixing. Our original submission included that feature,
as stated in Figure 3 line 6 and near the end of Section 2.5. We believe the source of the reviewer's confusion is that P3
requires strict fitness improvement when adding solutions to the pyramid, as stated in Figure 4 line 9 and near the beginning
of Section 3.

\item
\begin{quote}
The abstract states that P3 requires ``less wall clock seconds'' and ``is more effective than existing state-of-the-art techniques''.
\end{quote}

The abstract no longer mentions wall clock time.

\item
\begin{quote}
 Self-adaptation has been around since the '70's in Evolution Strategies.
\end{quote}

Quite true. We have updated to more appropriate references.

\item
\begin{quote}
RRHC is used very often in the past, you cannot refer to it in this way.
\end{quote}

We have updated the text to clarify that we are referring to a specific RRHC and not suggesting it was the first RRHC

\item
\begin{quote}
``Doerr et al. (2013) presented the first genetic algorithm to provably show the advantage of performing crossover on simple landscapes.''
I understand some people have a very restricted view on provably showing something but this statement is arguably wrong.
\end{quote}

We have made the claim more precise to better reflect their actual accomplishment.

\item
\begin{quote}
typo:  ``in a reduced the need for''
\end{quote}

Fixed

\item
\begin{quote}
It is not updating the model, it is completely rebuilding an $O(L^2)$ linkage tree model 
\end{quote}

Replaced ``updating'' with ``rebuilding''

\item
\begin{quote}
``Furthermore, unlike LTGA all solutions in the population are used to generate the model, not just the winners of a binary tournament''.
This is not a major design choice, this only influences the selection pressure. Actually, the most recent papers report all experiments with LTGA using the entire population instead of a set of selected solutions.
\end{quote}

As discussed in Section 2.5 we are aware that there are many variants of LTGA. However, for clarity and to perform experiments we had to choose a reference version, and Thierens and Bosman (2013) uses binary tournament. We updated the wording to indicate there are other versions.

\item
\begin{quote}
Fig. 4: ITERATE-P3, line 12: the model rebuilding should be stated here.
\end{quote}

Though we have stated previously that adding to the pyramid triggers model building, we have added text to the figure to restate it.

\item
\begin{quote}
We have seen problems where smallest first is not better. Basically any a priori ordering represents a bias to the LTGA search algorithm.
\end{quote}

Our argument is not that smallest first is the \textit{best}, only that it has desirable diversity preservation features which are problem independent. We have attempted to clarify this point.

\item
\begin{quote}
 ``This function modifies the results of Equation 4 to include plateaus of size s, introducing an exponential number of local optima in each trap.''
Exponential number ? Clarify with a small example of Deceptive Step Trap.
\end{quote}

We have added the following text for clarity. 
``With $k=7$ and $s=2$, as used in our experiments, all traps with 0, 1, 3, 5, and 7 bits set are local optima.
This means that half of all ways to set the trap are 1 bit local optima.
More generally, the number of local optima grows at $\Theta(2^{k-1})$.''

\item
\begin{quote}
HIFF: needs a reference to Watson
\end{quote}

Done

\item
\begin{quote}
Equation (6): what is A ?
\end{quote}

A= 10 (updated in paper)

\item
\begin{quote}
``To ensure each problem is challenging we chose a clause to variable ratio of 4.27''
Reference to support this number.
\end{quote}

Added 

\item
\begin{quote}
Figure 8: This is a confusing figure: please elaborate.
\end{quote}

It is a busy figure. We have worked to clarify the text describing the figure.

\item
\begin{quote}
``An important note is that each donation may range in size from a single bit up to N-1, with smaller donations far more likely to result in repeated attempts. As such this may cause some super-linear growth in P3, but its unlikely to be very high.''
Speculative ?
\end{quote}

Due to the tree structure there are more small clusters than large ones. The nested nature of clusters means that if most solutions agree for a large cluster, at least that many solutions must agree for both of its subset clusters. As to the super-linear growth, we draw that conclusion from Figure 8b and its discussion. We reworded this statement slightly to be more precise.

\item
\begin{quote}
``In total these runs took over 6 computing years to complete.''
\end{quote}

The quote is correct, but being taken slightly out of context. This is the report for \textbf{all} runs. In our original submission 95\% of all computing time was used by hBOA and Parameter-less hBOA. The other 4 algorithms combined took less than 100 days to complete. After fixing our LTGA inefficiency and rerunning on uniform, modern hardware, the non hBOA algorithms took in total 31 days of computing to complete 124 billion evaluations.

In rerunning our experiments we chose to ensure all experiments were performed in identical hardware. Unfortunately this means we are unable
to complete the largest problem sizes of hBOA and Parameter-less hBOA. This section has been updated accordingly.

\item
\begin{quote}
How can this be only a constant factor, when P3 uses a $O(L^2)$ operation each time an improved solution is found (= thousands of times) versus only a dozen $O(L^2)$ operations for LTGA ???
\end{quote}

This is covered in the original text, Section 2.4 and 2.5. However, to review,
the runtime bottleneck for LTGA is extracting pairwise frequency information which
requires $O(\mu N^2)$ time per generation. This happens approximately once every $\mu N$ evaluations,
meaning the time per evaluation is $(\mu N^2) / (\mu N) = O(N)$.
In P3, both entropy extraction and model rebuilding are $O(N^2)$ and must be done every $N$ evaluations,
meaning the time per evaluation is $N^2 / N = O(N)$. Therefore, excluding caveats mentioned in the paper,
the two techniques should be within a constant factor runtime of each other.
This relationship in general holds true for our updated experiments.


\item
\begin{quote}
``There is some evidence that even when compared on wall clock time, P3 performs at least as well as the best comparison techniques''
\end{quote}

Fixed based on the corrected code. Now ``on par'' instead of ``better.''

\item
\begin{quote}
``While asymptotically linear''   Linear in what ?
\end{quote}

Fixed

\item
\begin{quote}
 Do not refer to TRs, refer to actual publications.
\end{quote}

Replaced.

\item
\begin{quote}
Add a reference, important modifications of LTGA you use were first published in:...
\end{quote}

Done
\end{itemize}

\subsection{Reviewer 2}
\begin{itemize}

\item
\begin{quote}
My only gripe is that all algorithms are compared according to their median results and it would be nice to see at least a discussion of what this implies in terms of statistical significance.
\end{quote}

Median is the non-parametric alternative to mean, which requires data to be normally distributed.
Also, medians allow us to incorporate failures into the statistic. For example, if an algorithm fails due to
exceeding the evaluation limit, as long as the median run is successful the median is unaffected. If mean was used
it becomes very difficult to handle failures of that kind. We have added some short statements about this to the text.

We have also added statistical tests of data from Figure 6, Figure 7, and Figure 12 to Section 6.1, Section 7.1, and Section 9.2.

\item
\begin{quote}
Last sentence of section 1, penultimate clause doesn't have  verb in it. I suggest you rephrase as `` ...; analyse the algorithm complexity;''
\end{quote}

Fixed

\item
\begin{quote}
Around equation (1) you need to state what m(i), m(i,l), l' and l'' denote rather than leaving the reader to find those in the appendix.
\end{quote}

Done

\item
\begin{quote}
Section 8.2.1 uses the phrase ``significantly worse''.  This suggests that you have some specific confidence in the results according to some hypothesis testing method.  Please clarify or rephrase.
\end{quote}

You are correct, you cannot test that statistically. Rephrased to avoid that impression.

\end{itemize}

\section{Disagreements}

There is some disagreement, reflected in multiple comments, that local search is either unnecessary or an unfair advantage when used in P3. Our view is this.

We have explained that, for the particular situation in the bottom pyramid level,
before a randomly generated solution is added to that level, that solution should be hill climbed.
Our reasoning is straightforward; a randomly generated solution is unlikely to
have any useful linkage information to be incorporated into that level.
Once hill-climbed, that solution is more likely to provide useful linkage information to that bottom level.
This is true even for problems with large fitness plateaus, such as Deceptive Step Trap.
As such we feel the application of local search should be considered the standard approach
with P3, only to be removed under exceptional circumstances.

When we count evaluations and wall-clock time, we include the local search performance. That is, we are explicitly not allowing for partial evaluations, and count every local search evaluation P3 performs just like any other evaluation. Thus our timings and evaluations are fair and accurate.

Our job, as algorithm designers, is to compare our approach against the best possible competing algorithms
and report how our approach fares. We have taken, to the best of our ability using available published resources,
those best algorithms and compared them with P3 on a rigorous set of problems.
We went out of our way to tune these algorithms
so that we may compare on the best possible footing, giving them an advantage not available to P3.

However, adding initial local search as a tunable parameter to hboa and LTGA when partial evaluations are not available is not part
of existing work for either method. Doing so would also provide those techniques
with even more problem specific configuration. We welcome future work by other researchers exploring the possible effects of combining
local search with their techniques in this fashion.

\end{document}
