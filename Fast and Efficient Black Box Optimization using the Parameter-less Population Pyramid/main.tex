\documentclass[twoside]{article}
\usepackage{ecj,palatino,epsfig,latexsym,natbib}
\usepackage{url}
\usepackage{verbatim}
\usepackage{subcaption}
\usepackage[noend]{algpseudocode}

\newcommand{\includegraphicswide}[1]
{\includegraphics[width=.9\textwidth,height=\textheight,keepaspectratio]{#1}}

\newcommand{\includegraphicsfit}[1]
{\includegraphics[width=\columnwidth,height=\textheight,keepaspectratio]{#1}}

\usepackage{array}
\newcolumntype{L}{>{\centering\arraybackslash}m{1.9cm}}

%% do not add any other page- or text-size instruction here

\parskip=0.00in

\begin{document}

\ecjHeader{x}{x}{xxx-xxx}{200X}{TODO 45-character paper description}{B. W. Goldman and W. F. Punch}
\title{\bf Fast and Efficient Black Box Optimization using the Parameter-less Population Pyramid}  

\author{\name{\bf B. W. Goldman} \hfill \addr{brianwgoldman@acm.org}\\ 
        \addr{Department of Computer Science and Engineering, Michigan State University, 
        East Lansing, 48823, United States}
\AND
       \name{\bf W. F. Punch} \hfill \addr{punch@msu.edu}\\
        \addr{Department of Computer Science and Engineering, Michigan State University, 
        East Lansing, 48823, United States}
}

\maketitle

\begin{abstract}

TODO About 200 words.

\end{abstract}

\begin{keywords}

Genetic algorithms, 
linkage learning,
local search,
parameter-less.

\end{keywords}

\section{Introduction}
A primary purpose of evolutionary optimization is to efficiently find good solutions
to challenging real world problems with minimal prior knowledge about the problem itself.
This driving goal has created search algorithms which can escape user bias to create
truly novel results, sometimes publishable or patentable in their own right~\citep{kannappan:2014:humies}.
While it is not possible for any algorithm to do better than random search across all possible
problems~\citep{Wolpert:1997:nfl}, effectiveness can be achieved by assuming the search
landscape has structure and then biasing the algorithm toward exploiting that structure.

In evolutionary optimization, and genetic algorithms~(GAs) in particular, search is often
biased through parameters. This can be beneficial as it allows practitioners to inject their
knowledge about the shape of the search landscape into the algorithm.
However, the quality of solutions found, and the speed at which they are found, is strongly tied to setting these parameters
correctly~\citep{goldberg:1991:gasize}. As such expert knowledge or exceedingly
expensive parameter tuning~\citep{grefenstette:1986:optimalga} may be required to leverage
this feature to its fullest potential. Furthermore,  parameters such as population size, mutation rate, crossover
rate, tournament size, etc can have no clear relationship to the problem being solved, meaning even
domain experts may not understand how they will interact with the problem or with each other.

As such there have been periodic efforts to reduce or remove the need for parameter tuning.
\cite{Back:1992:selfadapt} introduced self-adaptive parameters, in which parameter values
were included in each solution's genome and underwent evolution. This allowed search
to optimize some of its own parameters resulting in a reduced the need for expert tuning.
\cite{harik:1999:parameterlessga} were able to design an entirely parameter-less GA by
leveraging schema theory and parallel populations. Unfortunately these methods were provably less efficient
than directly setting the parameters to optimal values~\citep{pelikan:1999:worstparameter-less}.

One area that has been very effective at reducing the number of algorithm parameters is
model based search. \cite{pelikan:2006:hboa}'s Hierarchical Bayesian Optimization
Algorithm~(hBOA) and \cite{thierens:2010:ltga}'s Linkage Tree Genetic Algorithm~(LTGA)
both only require a single parameter: population size. \cite{posik:2011:parameterless}
leveraged model building to create a fully parameter-less algorithm, but it is restricted to
only order-k, fully decomposable, noiseless problems.

Most recently \cite{goldman:2014:p3} introduced the Parameter-less Population Pyramid~(P3).
This method uses a pyramid structure of populations to combine model based search with local search
to achieve parameter-less optimization. Initial results suggest that unlike
previous parameter-less methods, P3 is actually more efficient than current state-of-the-art
parameterized search algorithms. In this work we shall extend these results to cover more
comparison algorithms, compare both efficiency in reaching the global optimum and intermediate
fitnesses, algorithm complexity analysis, and provide more in depth analysis of P3 itself.
\begin{comment}
Section~\ref{sec-optimizers}
explains how each of these algorithms, including P3, perform search. Section~\ref{sec-problems}
provides a description of each test problem. As hBOA and LTGA require a population size parameter
Section~\ref{sec-tuning} provides our methodology to ensure each is optimally tuned to each problem.
\end{comment}

\section{Comparison Optimizers}
\label{sec-optimizers}

In order to fully understand the effectiveness of a black box search algorithm, it is useful
to compare it with other similar algorithms. Therefore here we preset five advanced algorithms with
related features to P3. The Random Restart Hill Climber was chosen as an efficient form of repeated
local search, and because it shows the advantages of performing the more complex portions of P3.
The $1+(\lambda, \lambda)$ algorithm is the current best theory supported simple genetic algorithm
and its method of crossover is in some sense a macro-mutation just as in P3. hBOA and Parameter-less
hBOA are advanced model building search techniques which are very effective at learning complex
problem structure, designed to achieve similar goals as P3's linkage learning but using very different
methods. Finally LTGA represents the current state-of-the-art in black box search and is the origin
of P3's linkage learning and crossover methods.

Finally, only hBOA and LTGA require any parameters, with each of these only requiring a population
size. This makes knowing the optimal behavior of these algorithms much more tractable. All of the
algorithms are also gene order independent, fitness scale invariant, and unbiased. This means
for any problem the order in which problem variables appear in the genome can be changed
without changing the behavior of the search. The fitness can also be manipulated in any fashion
as long as the rank ordering of solutions is unchanged. These algorithms are also unaffected by the meaning
assigned to each bit, such that inverting a predetermined random subset of genes before evaluation
will not impact search efficiency.

\subsection{Random Restart Hill Climber}
\label{sec-hill-climber}
Perhaps the simplest black box search heuristic is stochastic local search, or hill climbing.
This optimization technique focuses on improving a single solution until it reaches a local
optimum. Here we use the first improvement hill climber defined by \cite{goldman:2014:p3}
and given in Figure~\ref{fig-hc}. This algorithm works by flipping each bit in a random
order, keeping modifications when fitness is improved, until doing so cannot result in
further fitness improvements.

\begin{figure}
  \begin{algorithmic}[1]
  \Procedure{Hill-Climber}{}
    \State $options \leftarrow [0 \dots N-1]$
    \State $tried \leftarrow \emptyset$
    \While{$|tried| < |options|$}
      \ForAll{$index \in shuffled(options)$}
        \If{$index \notin tried$}
          \State Flip bit $index$ in solution
          \If{solution's fitness increased}
            \State $tried \leftarrow \emptyset$
          \Else
            \State Revert change
          \EndIf
          \State $tried \leftarrow tried \cup \{index\}$
        \EndIf
      \EndFor
    \EndWhile
  \EndProcedure
\end{algorithmic}
  \caption{Hill climbing algorithm used to improve randomly generated solutions until no single
           bit change results in a fitness improvement.}
  \label{fig-hc}
\end{figure}

The hill climber requires an amortized cost of $O(1)$ operations per evaluation. In order to
terminate, at least one evaluation must be performed for each of the $N$ bits in the solution.
As such any operation that happens only once per search can be amortized over at least $N$
evaluations, covering the initialization of $options$.

With the use of proper hashing data structures, determining if $index \notin tried$ requires $O(1)$
time. While this operation may be called without a directly related evaluation, amortized cost
provides an upper bound. Consider that in order to add an $index$ into $tried$ an evaluation must
be performed. Lets say this evaluation pays for two checks of that $index$, one of which was used
during this loop. If no fitness improving move is found, the next time through the loop the second check
is used. A third check is only necessary if no fitness improving move was found for any index.
This is a contradiction as the loop terminates when no improving move was found. Therefore this check
can only be called twice per evaluation.

Due to its nature, this hill climber cannot escape basins of attraction. Once a solution is reached
such that none of the single bit neighbors are fitness improvements, search stops. As a result to
have any hope of solving multimodal problems this algorithm requires a restart mechanism. We have
chosen here to naively restart search from a random solution whenever a local optima is found. This
ensures that on all landscapes there is always a non-zero probability of search finding the global optimum.

\subsection{$1+(\lambda, \lambda)$}
\cite{doerr:2013:lambdalambda} presented the first genetic algorithm to provably show
the advantages of performing crossover on simple landscapes. This comparatively simple
algorithm maintains only a single individual and a self-controlled parameter $\lambda$.

Each iteration, the number of bits to flip is chosen from the binomial distribution $b\sim B(N, \frac{\lambda}{N})$,
where $N$ is the number of bits in the genome.
Next, $\lfloor\lambda\rfloor$ offspring are produced by flipping $b$ bits. The
best mutant then produces $\lfloor\lambda\rfloor$ offspring via uniform crossover with the original parent, such that each gene comes from the
mutant with probability $\frac{1}{\lambda}$. In the original algorithm the best
offspring produced by crossover then replaces the original parent if its fitness is no worse.
The $\lambda$ parameter, which is initialized to 1, is decreased if the offspring replaced
its parent and increased otherwise.

The original formulation was designed specifically for unimodal landscapes and as such were
not directly suitable for multimodal problems. \cite{goldman:2014:p3} extended $1+(\lambda, \lambda)$
to include random restarts. As search stagnates, the $\lambda$ parameter increases in value. Eventually
this results in $\lambda \ge N$ causing mutation to always flip all bits of the individual.
As this prevents any future improvement, whenever $\lambda \ge N$ search is restarted from a random solution with $\lambda$
reset to 1.

A few other efficiency modifications were also made. If there is a tie in crossover offspring fitness,
whichever has a larger hamming distance from the parent is retained. This encourages drifting across plateaus.
The ``mod'' control strategy proposed by \cite{doerr:2013:lambdalambda} was not used as it conflicted with
the random restart strategy.
If a crossover individual is identical to either of its parents, it is not evaluated.
If mutation produces an offspring which is better than the best crossover offspring, it is used to compare
against the original parent.


\subsection{Hierarchical Bayesian Optimization Algorithm}

\cite{pelikan:2006:hboa} used statistical principles in combination with a decision tree structure
to create the Hierarchical Bayesian Optimization Algorithm (hBOA). This method creates a model of
epistatic relationships between genes which is then used to stochastically generate new solutions.
Each generation a binary tournament with replacement is used to selection $\mu$ solutions from
the population. These solutions are then used to build the model, which in turn is used to generate $\mu$ new
solutions. The new solutions are then integrated into the population using restricted tournament
replacement.

Conceptually, the model built by hBOA is trying to infer rules of the form ``Given that this
subset of genes are set to these values, how frequently is gene $x_i$ set to value v?'' This can
be represented using a directed acyclic decision forest, with each tree in the forest representing one gene
in the solution. In the decision tree $T_i$, which is used to set the value of gene $x_i$,
each internal node represents previous decisions on how to set
some other gene $x_j$, with the children of that node representing how the decision was made. The
leaves of each tree give the probability that $x_i$ should be set to each possible value.
Therefore in binary alphabets binary trees with a single probability $p$ stored at each leaf
can be used, where $p$ is the probability $x_i$ should be set to one.

The forest is constructed iteratively, with each tree initially containing a single leaf
and with each leaf storing a pointer for each selected solution. Each iteration the algorithm considers
all possible ways of splitting an existing leaf using another gene $x_j$, such that solutions in the
leaf are moved to the newly created leaves based on their value for $x_j$. The general goal is to
separate the solutions such that all solutions with $x_i = 0$ move to one leaf while solutions with
$x_j = 1$ move to the other. Whichever split maximizes Equation~\ref{eq-hboa} is then kept.
This process continues until none of the splits result in a higher quality model than the current one. To favor compact
models, Equation~\ref{eq-hboa} is scaled by Equation~\ref{eq-hboa-stop}, which provides increased cost
for more total leaves.

\begin{equation}
  BDe(B) = \prod_{i=0}^{N}\prod_{l\in L_i} \frac{\Gamma(m'_i(l))}{\Gamma(m_i(l) + m'_i(l))}
  \prod_{x_i}\frac{\Gamma(m_i(x_i, l) + m'_i(x_i,l))}{\Gamma(m'_i(x_i,l))}
  \label{eq-hboa}
\end{equation}

\begin{equation}
  p(B) = c2^{-0.5(\sum_i|L_i|)log_2\mu}
  \label{eq-hboa-stop}
\end{equation}

While Equation~\ref{eq-hboa} is very complex and almost always results in near infinitesimal results, many
of the terms can be canceled out as it is only important to know the relative quality between two models:
$p(B)BDe(B) < p(B')BDe(B')$.
The outermost product of Equation~\ref{eq-hboa} iterates over all trees in the forest. However, each split
can modify only one of the trees and therefore the contribution of all others can be canceled. The middle
product is across all leaves in the tree. Again since only one leaf can be changed, all other terms can
be canceled. By convention hBOA uses uninformed Bayesian priors of $m'_i(l)= 2$ and $m'_i(x_i, l)=1$ for
binary alphabets. As $\Gamma(a) = (a-1)!$ this means the top term in the middle product and the bottom
term in the third product reduce to 1. The only remaining terms are then $m_i(l)$ and $m_i(x_i, l)$ which
represent the number of solutions which reached leaf $l$ and the number of solutions which reached leaf $l$
with a specific value for $x_i$, respectively.

Equation~\ref{eq-hboa-stop} can also be simplified when doing comparisons. If model $B'$ has exactly one more
leaf than model $B$ than the ratio $\frac{p(B)}{p(B')}$ simplifies to $2^{0.5 log_2\mu}$ regardless of
total model size.

All combined, this means the expression $p(B)BDe(B) < p(B')BDe(B')$ can be calculated using Equation~\ref{eq-hboa-final},
where $B'$ is different from $B$ by exactly 1 split, such that $l$ was split to create $l'$ and $l''$. Only splits that
satisfy this inequality are considered good enough to keep. Furthermore, whichever split maximizes the right side of the
Equation~\ref{eq-hboa-final} is chosen each iteration. Note that these factorials can be exceedingly large and
therefore it is imperative that implementations avoid rounding errors and overflows.

\begin{equation}
  2^{0.5 log_2\mu} < \frac{(m_i(l) + 1)!}{m_i(0, l)!m_i(1,l)!} \cdot
  \frac{m_i(0, l')!m_i(1,l')!m_i(0, l'')!m_i(1,l'')!}{(m_i(l') + 1)!(m_i(l'') + 1)!}
  \label{eq-hboa-final}
\end{equation}

Initially there are $\Theta(N^2)$ possible ways to split existing leaves, as each of the $N$ single node
trees can be split by any of the other $N-1$ genes. Each iteration a new edge is added to the decision
forest, meaning some of the previously tested splits cannot be used. For instance, if $T_i$, which is used
to decide the value of $x_i$, is split using the value of $x_j$, $T_j$ can no longer be split using $x_i$.
As a split creates two new leaves, $O(N)$ new potential splits must also be tested. Equation~\ref{eq-hboa-final}
parses all solutions which reach a leaf to count gene frequencies, requiring $\mu$ time.
The number of total leaves created depends heavily on the problem and $\mu$.
However, assuming no splits are accepted or that the cost of testing all future splits is less than
the initial $\Theta(N^2)$, constructing the model requires $\Omega(\mu N^2)$ time. Each
model is used to generate $\mu$ solutions, leading to a cost per evaluation of $\Omega(N^2)$.

To generate a solution, the value of each gene $x_i$ is set using its corresponding decision tree $T_i$. Because
the forest is directed acyclic, there must be an ordering of $T_i$ such that before $T_i$ is executed all
$x_j$ it uses to make decisions have already been set. As such, previous decisions made by other trees
are used to follow each $T_i$ until a leaf is reached. The value of $x_i$ is then set based on the
probably that other solutions reached that leaf with each value of $x_i$.

To perform replacement, hBOA uses restricted tournament replacement. After each new solution is generated
and evaluated, $w$ solutions are chosen at random from the population, where $w=\min\{N, \frac{\mu}{20}\}$.
Whichever of the $w$ solutions is the most genetically similar to the offspring is compared with the offspring.
If the fitness of the offspring is no worse, it replaces the genetically similar member of $w$, otherwise the
offspring is discarded. This method is designed to preserve genetic diversity as only genetically similar
solutions must compete on fitness.

To model properly, hBOA is designed to work with large population sizes, resulting in a large number of
evaluations per generation. As hBOA utilizes explicit diversity maintenance, standard methods for determining
convergence are not considered very accurate. Therefore the authors suggest that an hBOA run should be
terminated after performing generations equal to $N$.

Like other model based techniques, hBOA has very few parameters. There is no mutation or crossover,
and modeling does not rely on any explicit parameters. Solution selection, generation, and replacement
are all derived from the population size, which must be set by the user.

\subsection{Parameter-less hBOA}
Using the methods first introduced by \cite{harik:1999:parameterlessga} for the Parameter-less GA,
\cite{pelikan:2004:parameterlesshboa} created Parameter-less hBOA which automatically scales its
population size to fit the problem. This is done by maintaining a list of concurrent populations
using exponentially scaled population sizes.

A run of Parameter-less hBOA starts with a single population of size $\mu_0$, conventionally set
to $\mu_0=10$. After two generations are performed, a new population of size $\mu_1 = 2\mu_0$ is created
and performs a generation. Evolution then continues with the $\mu_0$ population performing two generations
for each one performed by $\mu_1$. Each time population $\mu_i$ performs its second generation a new population
$\mu_{i+1}=2\mu_i$ is created, which performs generations half as often as $\mu_i$. In this way an infinite number of
parallel population can be simulated, with each population receiving the same number of total evaluations.

In all other aspects each population is identical to an hBOA population using a fixed $\mu$. No search information
is shared among populations, and each search is independently terminated. As such Parameter-less hBOA cannot
perform better than hBOA using the optimal population size for a given instance, as it must also spend evaluations
on populations of different sizes. This inefficiency is bounded by a log multiple of the total number of
evaluations~\citep{pelikan:1999:worstparameter-less}.

\subsection{Linkage Tree Genetic Algorithm}
~\cite{thierens:2010:ltga} introduced the Linkage Tree Genetic Algorithm (LTGA) which automatically
detects and exploits problem epistasis by examining pairwise gene entropy. Due to its enhanced
ability to preserve high fitness gene subsets, LTGA was able to outperform state of the art
GAs across many benchmarks. Since its first proposal, many variants of LTGA have been
proposed~\citep{goldman:2012:ltga} so for clarity we have chosen the version
presented by \cite{thierens:2013:ltgahiff} as our model. This variant was chosen as it is
modern and appears to approach the consensus in the literature.

LTGA's effectiveness comes from its method of performing crossover. Instead of blindly
mixing genes between parents, LTGA attempts to preserve important interrelationships
between genes. Before performing any crossovers in a generation, LTGA first builds
a set of hierarchical gene clusters which are then used to dictate how genes are mixed
during crossover.

\begin{figure}
  \begin{algorithmic}[1]
  \Procedure{Cluster-Creation}{}
    \State $unmerged \leftarrow \{\{0\}, \{1\}, \{2\}, \dots, \{N-1\}\}$
    \State $useful \leftarrow unmerged$
    \While{$|unmerged|>1$}
      \State $C_i, C_j \leftarrow \min_{C_i,C_j \in unmerged} D(C_i, C_j)$
      \State $unmerged \leftarrow unmerged - \{C_i, C_j\} + \{C_i \cup C_j\}$
      \State $useful \leftarrow useful + \{C_i \cup C_j\}$
      \If{$D(C_i, C_j) = 0$}
        \State $useful \leftarrow useful - \{C_i, C_j\}$
      \EndIf
    \EndWhile
    \State Order $useful$ based based on last merged first\label{fig-cluster-creation-ordering}
    \State Remove largest cluster from $useful$

    \Return $useful$
  \EndProcedure
\end{algorithmic}
  \caption{Algorithm describing how LTGA creates clusters using Equation~\ref{eq-distance}
           for a population. $unmerged$ and $useful$ are ordered sets of sets of gene loci.}
  \label{fig-cluster-creation}
\end{figure}

Figure~\ref{fig-cluster-creation} provides the agglomerative method LTGA uses to create gene clusters.
This algorithm will create a tree of sets, such that the leaves of the tree contain a single gene and
nodes are the union of their children's sets. These sets are then used by crossover to specify epistatic
relationships which should be preserved.
The process begins by creating the set of sets $unmerged$ which tracks all top level clusters. Initially
$unmerged$ contains single member sets for each gene. After each iteration the two sets with the minimum average pairwise
distance (given in Equation~\ref{eq-distance}) are merged to create a single cluster. This process is repeated
until only a single set remains in $unmerged$ which contains all of the genes in the genome.

\begin{equation}
  D(C_i,C_j) = \frac{1}{\left | C_i \right |\cdot \left |C_j \right|}\sum_{c_i \in C_i}\sum_{c_j \in C_j}
  2 - \frac{H(c_i) + H(c_j)}{H(c_i \cup c_j)}
  \label{eq-distance}
\end{equation}
\begin{equation}
  H(c) = -\sum_{s\in S} p_c(s)\log(p_c(s))
  \label{eq-entropy}
\end{equation}

Throughout this process $useful$ tracks the set of all gene clusters which should be preserved for use by crossover.
This set begins with all genes in separate clusters, and each time a new cluster is created it is added to $useful$.
However, not all clusters are necessarily worth keeping. For instance, in all versions of LTGA the cluster
containing all genes is removed from $useful$ as preserving all genes during crossover can only create clones.
\cite{thierens:2013:ltgahiff} extended this removal to include any unsupported subsets. If the pairwise distance
between two clusters is 0, this means there are no individuals in the population which disrupt the relationships between the two
clusters. Therefore when performing crossover there is no reason to believe a fitness improvement can be achieved
by breaking the stored pattern. As such a cluster is only kept if its direct superset has a non-zero distance.

\cite{thierens:2013:ltgahiff}'s version of LTGA does not use the entire population when determining pairwise entropy.
Instead, binary tournament is used to select half of the population. This is done to ensure the model is built
using only high quality solutions, even during the first generation.

In order to efficiently perform clustering, a pairwise gene frequency table is constructed.
Equation~\ref{eq-entropy} uses how frequently each of the four possible string values for each
pair of genes occurs in the selected solutions. Extracting this information requires $O(\mu N^2)$
time, where $\mu$ is the population size and $N$ is the genome size. The process of converting
this pairwise frequency information into clusters can be achieved in $O(N^2)$ using the bookkeeping
methods presented by \cite{gronau:2007:upgma}. This cost is performed only once per generation,
and is then used to perform approximately $O(\mu N)$ crossover evaluations. As a result, the amortized cost of
LTGA's model building is $O(N)$.

\begin{figure}
  \begin{algorithmic}[1]
  \Procedure{Cluster-Usage}{}
    \ForAll{$C_i \in useful$}
      %\ForAll{$d \in shuffled(P_i)$}
        \State $d \leftarrow rand\_choice(P)$\label{fig-cluster-usage-donate}
        \State Copy $d$'s gene values for $C_i$ into solution
        \If{solution was changed}
          \If{solution's fitness decreased}
            \State Revert changes
          \EndIf
          %\State \textbf{break}
        \EndIf
      %\EndFor
    \EndFor
  \EndProcedure
\end{algorithmic}
  \caption{Algorithm describing how clusters are used to perform crossover.}
  \label{fig-cluster-usage}
\end{figure}

Figure~\ref{fig-cluster-usage} describes how the identified clusters are used by crossover to preserve
gene linkage while still exploring the search space. Unlike more traditional crossover methods, LTGA
crosses each individual with the entire population. Also, to produce a single offspring, multiple evaluations
of the fitness function are performed.

Each generation, each individual in the population undergoes crossover. In a single crossover event, each
cluster of genes $C_i$ in $useful$ is applied in last merged first order as a crossover mask. A random donor $d$
is chosen from the entire population (not just the model selected population), and $d$'s gene's for $C_i$ are copied
into the working solution. If a modification is made, an evaluation is then performed. If the crossover
resulted in no worse fitness then the changes are kept. This allows for neutral drift across plateaus.
The resulting solution, which must be at least as fit as its parent, is then
copied into the next generation.


In total each individual can cause up to $|useful|$ evaluations. If all clusters were kept, even those deemed
unhelpful, and all donations were evaluated, even those which did not change any genes, then \Call{Cluster-Usage}{}
would perform exactly $2N-2$ evaluations for each of the $\mu$ solutions in the population. This provides the amortizing evaluations
required to make clustering only $O(N)$ operations per evaluation. However, by skipping some evaluations, its
possible that clustering may be super-linear.

LTGA has no explicit form of diversity control and has no method for introducing new genetic information once
the population has converged. Therefore an LTGA run is considered converged once two consecutive populations
contain the same unique solutions.

By design, LTGA only has a single parameter: population size. LTGA uses no mutation, and crossover is defined
in terms of the clustering algorithm. Selection between generations is fully elitist and embedded in the crossover,
with selection of model building solutions fixed to a binary tournament. Neither \Call{Cluster-Creation}{} nor
\Call{Cluster-Usage}{} rely on parameter values. LTGA does not provide any method for controlling or setting
the population size, and uses a fixed user specified size for each generation.

\section{Parameter-less Population Pyramid}
\label{sec-p3}
\cite{goldman:2014:p3} introduced the Parameter-less Population Pyramid (P3) as a method for
performing optimization which does not require the user to provide any parameters. This is
achieved by combining efficient local search with the model building methods of LTGA using
an iteratively constructed hierarchy of populations.

The high level algorithm of P3 is presented in Figure~\ref{fig-p3}. Unlike more traditional
GAs, P3 does not follow a generational model. Instead, it maintains an iteratively
expanding pyramid of expanding populations. Each iteration, a new random solution is generated.
This solution is brought to a local optimum using the hill climbing algorithm in Figure~\ref{fig-hc}. If that
local optimum has not yet been added to the pyramid, the solution is added to the lowest
population $P_0$.

Next, the solution is iteratively improved by applying LTGA's crossover algorithm (Figure~\ref{fig-cluster-usage})
with each population $P_i$ in the pyramid. If this process results in a strict fitness improvement and has
created a solution not yet stored in the pyramid, it is added to the next highest pyramid level $P_{i+1}$.
If $P_{i+1}$ does not yet exist, it is created. In this way populations in the pyramid expand over time,
and the number of populations stored increases over time. Initially the pyramid contains no solutions
or populations, meaning the user does not need to specify a population size.


\begin{figure}
  \begin{algorithmic}[1]
  \Procedure{Iterate-P3}{}
    \State Create random solution
    \State Apply hill climber
    \If{solution $\notin hashset$}
      \State Add solution to $P_0$
      \State Add solution to $hashset$
    \EndIf

    \ForAll{$P_i \in pyramid$}
      \State Mix solution with $P_i$
      \If{solution's fitness has improved}
        \If{solution $\notin hashset$}
          \State Add solution to $P_{i+1}$
          \State Add solution to $hashset$
        \EndIf
      \EndIf
    \EndFor
  \EndProcedure
\end{algorithmic}
  \caption{One iteration of P3 optimization. $pyramid$ is an
           ordered set of populations and $hashset$ is a set
           of all solutions in $pyramid$.}
  \label{fig-p3}
\end{figure}

To accommodate P3's unique population structure, some of LTGA's clustering procedures were adapted. In LTGA,
clusters are identified at the start of each generation, with those clusters used
to create all solutions produced in that generation. As P3 does not perform serial
generations, this process had to be modified. Instead, P3 updates the model each
time a solution is added to a population. Furthermore, unlike
LTGA all solutions in the population are used to generate the model, not just the
winners of a binary tournament. This selection process is not necessary as the
worst solutions in the pyramid are already high quality due to local search.
Using local search in LTGA was examined by \cite{bosman:2011:lsbbo} and found
to provide no significant improvement. A likely cause was that that study applied
local search to every solution, not just the initial population, resulting in significant overhead.

Beyond the changes in population structuring, P3 also has slight modifications to
LTGA's version of \Call{Cluster-Creation}{} and \Call{Cluster-Usage}{}.
P3 changes Line~\ref{fig-cluster-creation-ordering} in Figure~\ref{fig-cluster-creation}
from \emph{last merged first} ordering to \emph{smallest first} ordering.
This method applies gene clusters during crossover based on how many genes
are in each cluster\footnote{Ties are broken randomly.}, and not on how tightly linked those genes are.
\cite{goldman:2012:ltga} found that this alternative was better at preserving
diversity, and therefore required smaller populations, but has  not been integrated into
our chosen canonical LTGA variant.

P3 also modified Line~\ref{fig-cluster-usage-donate} in Figure~\ref{fig-cluster-usage}.
Instead of choosing a single genetic donor for each cluster, P3 iterates over the
population in a random order until a solution in the population is found which
has a least one gene different for that cluster of genes from the improving solution.
This process increases the likelihood of an evaluation being performed for every cluster,
and helps test rare gene patterns in the population.

In LTGA the cost of rebuilding the model is $O(\mu N^2)$ as it must collect pairwise
gene frequency information for all $\mu$ solutions in the population. P3 does not store
a single population, and it does not have a fixed $\mu$ size for any population. However,
each time a solution is added to the population, it requires $O(N^2)$ time to update
the table of pairwise frequencies and another $O(N^2)$ time to rebuild the linkage model.
The model is then used immediately to perform up to one evaluation for each of the up
to $2N-2$ clusters. Just as in LTGA, if no evaluation shortcuts were made, P3
has an amortized cost of $O(N)$ modeling cost per evaluation. While P3 does
rebuild the model more frequently per solution in the population, it also performs
a number of local search evaluations which are quite efficient, meaning theoretical
comparisons of their speed are difficult to perform. As a final note, P3's repeated
attempts to find a useful donation make it less likely than LTGA to skip evaluations,
but has an added cost to find these donations. Repeated donations could require as much
as $O(\mu)$ attempts per evaluation, but experimental evidence suggest that this
operation actually saves more overhead than it costs by increasing the number of evaluations
per model rebuild.

Each of the pieces of the P3 algorithm were selected not just for their stand alone efficacy,
but for the ways in which they interact. By using the hill climber to ensure each solution
is at a local optima, the underlying pairwise relationships in the problem are exposed. As
a result, detecting clusters for use by crossover is much more effective. The crossover operator
is extremely elitist, as each gene donation must result in no fitness loss, and a solution must
strictly improve to be added to the next level of the pyramid. This is balanced by continual
integration of new random solutions. Furthermore, each random restart decreases the probability
of spurious linkages caused by shared ancestry. This diversity is further preserved by applying
gene clusters in smallest first order during crossover as this reduces the probability of genetic
hitchhikers.

Other algorithms have proposed using multiple concurrent populations.
\cite{hornby:2006:alps} had a hierarchy of populations with solutions periodically advancing
upward. This allows for continuous integration of diversity as the lowest population is reseeded
with random solutions. However, this method resulted in increased parameterization as not only was
a population size required, parameters for how frequently generations advanced between levels and
how many total levels to have were added. \cite{harik:1999:parameterlessga} used multiple independent
populations of different sizes as a method for removing the population size parameter, but
doing was provably less efficient than using an optimal population size as no information is shared
between the populations.

\section{Problem Descriptions}
\label{sec-problems}

\subsection{Single Instance Problems}
Understanding how a stochastic search algorithm will behave on arbitrary and complex
search landscapes can be exceedingly difficult. Therefore a common practice for
algorithm understanding is to perform search on well defined, well understood
landscapes. To be of interest these landscapes need to represent interesting
and important aspects of real world problems.

Perhaps the most common such landscape is the Deceptive Trap problem \citep{goldberg:1991:gasize}.
In this landscape the genome is broken up into $k$ bit non-overlapping subproblems referred
to as traps.
Each subproblem is scored using Equation~\ref{eq-deceptive-trap}, where $t$ is the number of
bits in the trap set to 1. To global optimum in each trap is a string of all 1s, while all
other solutions lead to a local optima of all 0s. As a result this problem tests an algorithm's
ability to overcome $k$ sized deception and is commonly used to determine how effective crossover
is at preserving building blocks. Any crossover event that mixes bits from different parents in
the same trap will likely result in that trap being optimized to the local optima. For our experiments
we chose $k=7$ to ensure highly deceptive traps.

\begin{equation}
   trap(t) = \left\{
     \begin{array}{rl}
       k-1-t, &  t<k\\
       k,   &  t = k
     \end{array}
   \right.
  \label{eq-deceptive-trap}
\end{equation}

\cite{goldman:2012:ltga} found that mixing local search with linkage learning rendered the Deceptive
Trap problem trivial. This is because local search is able to optimize each trap to one of the two
optima (all 1s or all 0s), and then linkage learning can have perfect knowledge of gene interactions.
In order to make the problem more difficult yet still deceptive, the authors proposed the Deceptive
Step Trap problem, given in Equation~\ref{eq-deceptive-step-trap}. This function modifies the results
of Equation~\ref{eq-deceptive-trap} to include plateaus of size $s$, introducing an exponential number
of local optima in each trap. As a result the Deceptive Step Trap is much more challenging for linkage
learning techniques, while still being highly deceptive. We chose to use $s=2$ to create the maximum
number of local optima.
\begin{equation}
   step\_trap(t) = \left \lfloor \frac{(k-s)\pmod{s} + trap(t)}{s} \right \rfloor
  \label{eq-deceptive-step-trap}
\end{equation}

Another challenging aspect of landscapes can be higher order relationships. The Hierarchical If
and only If (HIFF) problem is designed to capture the difficulties of this class of problem.
In HIFF the genome is broken up into a complete binary tree, such that each gene appears in exactly one
leaf and each internal node is the subset of genes contained in its children. If all genes represented
in a node of the tree are set to the same value, they score equal to the size of the set. In this way
small subsets lead toward solutions to larger subsets. However a node can score if all genes are either
all 1s or all 0s, meaning that to solve higher order subproblems it is necessary to perform crossovers
that preserve lower order solutions. This problem is a natural fit for LTGA as the linkage tree can
perfectly duplicate the problem's true relationships \citep{thierens:2013:ltgahiff}.

As a final class of well known problems, we have chosen to borrow the Rastrigin problem from real valued
optimization. This problem's landscape, determined by Equation~\ref{eq-rast}, is highly multimodal caused by
the oscillating cosine function. \cite{goldman:2014:p3} proposed the Discretized Rastrigin problem, such
that each floating point $x_i$ in Equation~\ref{eq-rast} is encoded using a 10 bit gray code.
\begin{equation}
  An + \sum_{i=1}^{n}\left [ x_i^2-A\cos (2\pi x_i) \right ] \forall x\in [-5.12,5.12]
  \label{eq-rast}
\end{equation}

\subsection{Randomly Generated Problem Classes}
While well defined landscapes can provide specific insights into how an algorithm works,
their static nature can create misleading results. Specifically algorithm quality might
be very fragile such that it is only effective at searching well behaved landscapes. A more realistic
test of an algorithm's black box effectiveness is to work with randomly generated instances
drawn from a problem class. When tested over a sufficiently large sample it is then possible
to draw more general conclusions about the algorithms effectiveness. The challenge with these
landscapes is determining the global optimum to know if an algorithm was successful.

Perhaps the most common model for generating random rugged landscapes is the NK model. An
NK Landscape determines the fitness of each gene based on epistatic relationships with $K$
other genes in the genome. This fitness is specified using a randomly generated table of
fitness values, were each possible combination of the $K+1$ genes is mapped to some floating
point value $[0-1]$. In unrestricted NK landscapes the relationships between genes are also
randomly chosen and as a result finding the global optimum is $NP$-Hard for $K>1$. However,
if epistasis is instead set such that each gene depends on the $K$ directly following it
in the genome the solution can be found in polynomial time~\citep{wright:2000:solvingnk}.
These Nearest Neighbor NK landscapes are therefore ideal for search algorithm testing.
For all of our experiments we fixed $K=5$ to ensure highly rugged landscapes.

\cite{saul:1994:spinglass} presents a combinatorial benchmark problem derived from physics:
Ising Spin Glasses. A spin glass is defined by interaction terms between variables with
the genome encoding the sign for each variable and a goal to minimize the total interaction value.
Similar to NK Landscapes the general class is $NP$-Hard to optimize, but the $2D\pm J$ subset of
Ising Spin Glasses can be polynomially solved.\footnote{\url{http://www.informatik.uni-koeln.de/spinglass/}}

As our final class of randomly generated problems we chose the Maximum Satisfiability (MAX-SAT) problem.
Related to the more common 3-SAT problem, a MAX-SAT instance is defined by a set of three term clauses.
Each term is a randomly chosen variable, which may also be negated. A clause scores if an and only if
at least one term in the clause evaluates to true. In order to make MAX-SAT instances with a known global
optimum \cite{goldman:2014:p3} proposed constructing clauses around a fixed solution. In this way the
signs of the terms are set to ensure the target solution satisfies the clause. To ensure each
problem is challenging chose a clause to variable ratio of $4.27$.

\section{Comparison Algorithm Parameter Tuning}
\label{sec-tuning}
While four of the six algorithms in our experiments do not require any user specified parameters,
hBOA and LTGA both use a population size parameter. To ensure these techniques are not unfairly
handicapped, we have extensively tuned each using the bisection method \citep{sastry:2001:bisection}
to determine the optimal population size for each size of each problem.
Extended by \cite{goldman:2012:ltga} this method iteratively
doubles the population size until some success criteria is met and then performs bisection
between the lowest successful and highest unsuccessful sizes. Thus the minimum population size
which meets the success criteria is found. \cite{goldman:2014:p3} proposed a success criteria
of performing $r$ successful runs in a row, such that the expected failure rate can be bounded
above by $\frac{3}{r+1}$~\citep{jovanovic:1997:ruleofthree}. As P3 and the other three algorithms
do not prematurely converge, we chose $r=100$ to similarly ensure hBOA and LTGA almost never do.

\section{Finding the Global Optimum}
\label{sec-optimum}

\begin{figure}
  \begin{center}
  \includegraphicsfit{evals-to-success}
  \end{center}
  \caption{Comparison of the median number of evaluations to reach the global optimum for
           the six different optimization methods with respect
           to problem size.  If the median run did not reach the global optimum no data element
           is shown.  Results given on a log-log scale.}
  \label{fig-evals-to-success}
\end{figure}

The goal of black box optimization is to quickly and reliably obtain high quality solutions. In this
section we will compare how many evaluations each algorithm requires to find the global optimum on
each problem.

Figure~\ref{fig-evals-to-success} shows the median number of evaluations required by each of the six
algorithms to find the global optimum for multiple sizes of each problem.  Each data point in
Figure~\ref{fig-evals-to-success} represents the median of 100 runs, where unsuccessful runs
are treated as requiring more evaluations than any successful run. If the median run was not successful
no point is shown.

The Random Restart Hill Climber and $1+(\lambda, \lambda)$ are both relatively effective on small problem
sizes, but have difficulty scaling to larger problems.  Only on MAX-SAT are these optimizers competitive
at larger tested problem sizes. However this may just be an artifact of that the largest tested MAX-SAT
was an order of magnitude smaller than the largest size tested for most other problems. On the Deceptive
Trap and Deceptive Step Trap problem $1+(\lambda, \lambda)$ has some other interesting results. For
problems with 4 or less traps it performs significantly better than any other algorithm. This is likely
due to an artifact of the landscape as $1+(\lambda, \lambda)$ is able to overcome deception by probabilistically
flipping entire traps. For larger problems it becomes increasingly unlikely that mutation and crossover can
create enough changes to an individual trap to overcome deception without negatively changing others. This
behavior also causes high variance in success rate, as evident by the occasional successes on large problems.
This ability to successfully integrate modifications of more than a single bit generally appears to lead $1+(\lambda, \lambda)$
outperform the Hill Climber on all problems except Nearest Neighbor NK.

As expected, Parameter-less hBOA was generally efficient than hBOA using an optimal population size. The slopes
of the two lines are relatively similar across all problems, meeting with the theoretical expectation that
Parameter-less hBOA is no more than a logarithmic factor worse than hBOA.  The only problem which reversed
this trend was MAX-SAT, where Parameter-less hBOA actually outperformed hBOA. A potential explanation for
this upset is that hBOA is using the optimal population size for the problem class, not for each instance.
Therefore hBOA must use a population size sufficiently large to solve the hardest instances in the class.
Parameter-less hBOA on the other hand is able to solve easy instances using much smaller population sizes.
The net effect is therefore that by being more adaptable Parameter-less hBOA is able to achieve better median
results.

Considering how different hBOA and LTGA are in performing optimization, it is somewhat surprising how similar
their results are on HIFF. hBOA utilizes Bayesian decision trees to stochastically generate solutions while
LTGA uses a linkage tree to iteratively modify subsets of genes in an existing solution. On this problem both
methods likely have the same two limiting factors: initial diversity and generational turnover. In order to
ensure convergence to the global optima, both methods rely on initial populations which are sufficiently large.
Both need to preserve enough diversity to ensure higher order building blocks can be created later. Both methods
also build models once per generation. On HIFF detecting higher order relationships is very unlikely until lower
order subproblems have been optimized. As a result both LTGA and hBOA are unlikely to solve more than one order
of subproblem per generation. Therefore both require a number of generations based on the number of subproblem sizes
in the problem.

LTGA, as the current state-of-the-art, was generally the fastest to find the global optimum of all comparison
algorithms. This is especially true on the four single instance problems where it often found the global
optimum in a lower order of complexity.  However on Nearest Neighbor NK and Ising Spin Glasses LTGA appears
to perform worse than hBOA.  This may be due to LTGA's inability to represent the overlapping nature of these problems.
On MAX-SAT LTGA only outperforms hBOA. Again, the likely cause to this is LTGA's requirement to have a
population size sufficiently large to solve the hardest problem in each class, reducing its effectiveness
on instances of median difficulty.

\begin{figure}
  \begin{center}
  \includegraphicsfit{evals-to-success-range}
  \end{center}
  \caption{Comparison of the upper and lower quartiles of evaluations required
           to reach the global optimum for P3 and LTGA with respect to problem size.}
  \label{fig-evals-to-success-range}
\end{figure}

Across all problem and problem sizes P3 generally finds the global optimum using less evaluations than
all of the comparison algorithms.  On Deceptive Trap P3 significantly outperforms both hill climbing
and LTGA by combining the best features of each, using hill climbing to optimize each trap and crossover
to perfectly mix traps. Deceptive Step Trap less amenable to local search with P3's quality effectively
tied with both LTGA and hBOA. The slight improvement here may be due to P3's ability to solve some runs
using smaller population sizes than LTGA and hBOA due to lucky initialization. Using local search on HIFF
likely saves P3 a generation over LTGA and hBOA as all pairwise subproblems will already be solved. Combined
with P3's ability to add diversity only as needed this likely explains why P3 is able to solve HIFF a constant
multiple faster than either population based technique. P3 likely solves its median run of Nearest Neighbor
NK and Ising Spin Glasses faster than both hBOA and LTGA again by using population sizes fit to each instance
and not fit to the entire problem class. On MAX-SAT, where the most effective solvers are Hill Climber and
$1+(\lambda, \lambda)$, P3 outperforms both likely due to its use of local search with a better ability to
overcome deception.

To more fully examine the hypothesis that its P3's ability to scale to differences in problem difficulty,
Figure~\ref{fig-evals-to-success-range} shows the upper and lower quartiles for P3 and LTGA. On each problem
LTGA has a much smaller difference between its best and worst runs. This makes sense as LTGA uses the same
population size regardless of instance and progresses search generationally. In contrast P3 has a much
higher split, with some runs finishing very quickly. This added variability is generally on only the lower
end as P3's upper quartile is only worse than LTGA's upper quartile on Deceptive Step Trap and Nearest
Neighbor NK. A possible explanation for this is that as P3 has not pre-learned the amount of diversity required
to solve all instances does not take as risky behavior as LTGA.

\begin{comment}
\begin{table}
	\centering
	\begin{tabular}{|c|r|r|r|r|r|r|r|r|}
	  \cline{2-9}
	  \multicolumn{1}{c|}{} & \multicolumn{2}{L|}{Deceptive Trap} & \multicolumn{2}{L|}{Deceptive Step Trap}
	                        & \multicolumn{2}{L|}{HIFF} & \multicolumn{2}{L|}{Rastrigin} \\ \cline{2-9}
	  \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{a} & \multicolumn{1}{c|}{b}
	                        & \multicolumn{1}{c|}{a} & \multicolumn{1}{c|}{b}
	                        & \multicolumn{1}{c|}{a} & \multicolumn{1}{c|}{b}
	                        & \multicolumn{1}{c|}{a} & \multicolumn{1}{c|}{b} \\ \hline
	                          % DT                    % DST                    % HIFF                  % Rast
	  {Hill Climber}          &$<$0.00 &        11.10 &$<$0.00 &        10.60 &$<$0.00 &         9.45 &$<$0.00 &         5.50 \\ \hline
	  {$1+(\lambda,\lambda)$} &   0.02 &         3.66 &$<$0.00 &         6.69 &$<$0.00 &         6.55 &$<$0.00 &         4.50 \\ \hline
	  {hBOA}                  &  18.16 &         2.28 &  40.38 &         2.07 &   4.60 &         1.82 &   3.50 &         2.29 \\ \hline
	  {Parameter-less hBOA}   &  16.92 &         2.72 &  28.71 &         2.59 &   4.94 &         2.25 &   3.55 &         2.61 \\ \hline
	  {LTGA}                  & 324.88 & \textbf{1.28}& 165.27 &         1.72 &   7.02 &         1.66 &  21.05 & \textbf{1.43}\\ \hline
	  {P3}                    &  68.80 &         1.38 & 354.69 & \textbf{1.56}&   4.06 & \textbf{1.59}&   5.29 &         1.59 \\ \hline
  \end{tabular}
	\caption{Results of fitting $y=ax^b$ model where $y$ is evaluations to success
	         and $x$ is problem size. Bold entries are the statistically lowest growth rates.}
	\label{table-bigo}
\end{table}

TODO Table listing empirical $O(N)$ times for each technique on each problem. Include
statistical comparison of log-log regression lines.

\end{comment}

\section{Fitness Over Time}
\label{sec-overtime}
\begin{figure}
  \begin{center}
  \includegraphicsfit{fitness-over-time}
  \end{center}
  \caption{Compares the median best fitness reached during search for each of the six optimization methods.}
  \label{fig-fitness-over-time}
\end{figure}

For some applications, finding the global optimum is less important than finding good solutions quickly. Therefore
we examine this behavior in Figure~\ref{fig-fitness-over-time}. At regular intervals during optimization
Figure~\ref{fig-fitness-over-time} shows the median of the best fitnesses found by that point of search across
100 runs. For each problem we show the largest problem size for which we were able to successfully set a population
size for hBOA, but the trends shown are representative of all larger problem sizes. The maximum reporting interval
is set to include the slowest P3 run to reach the global optimum.

At almost every recording interval for every problem hBOA has the worst fitness. The only exceptions come when
the total number of evaluations performed is significantly less than the genome size hBOA reaches better fitness
than the Hill Climber, $1+(\lambda, \lambda)$, and P3. This early in search hBOA and LTGA are both still evaluating
individuals in their randomly generated initial populations, while the three techniques worse than hBOA have started
from a single solution and are attempting to optimize. Of the methods, hBOA uses by far the largest population sizes,
about 55 times larger than LTGA. As such it makes sense that hBOA would take a long time to being to reach high
quality solutions.

Parameter-less hBOA generally reports higher fitness than hBOA when using the same number of total evaluations. This makes
sense as Parameter-less hBOA's use of small populations allows it to begin to optimize long before hBOA has even evaluated
its entire initial population. However, this trend is reversed after a sufficient number of evaluations, as is apparent
on Deceptive Step Trap and Ising Spin Glasses. This makes sense as hBOA's population is set to quickly find the global
optimum.

Early in optimization P3 and the Random Restart Hill Climber have effectively identical quality. This makes sense
as until P3 performs two restarts it is performing the same evaluations as the Hill Climber. Once P3 begins
performing crossover it immediately improves over the Hill Climber. As such P3 is likely better than a simple
hill climber regardless of how long each technique is run and irrespective of how high quality the solution
found has to be.

Perhaps the most striking result is the quality of $1+(\lambda, \lambda)$. Until quite far into search this
method performs better than both LTGA and hBOA. Given sufficient evaluations $1+(\lambda, \lambda)$ outperforms
Random Restart Hill Climbing on all 7 problems. For brief periods in the middle of search it performs the
best of all techniques on the Deceptive Step Trap, HIFF, Ising Spin Glass, and MAX-SAT problems. A likely
explanation is $1+(\lambda, \lambda)$'s ability to efficiently incorporate gene modifications of larger
than one bit. This allows it to overcome the plateaus in Deceptive Step Trap, solve medium sized subproblems
in HIFF, flip the signs on multiple adjacent bits in Ising Spin Glass, and cross plateaus in MAX-SAT. However,
this method is not enough to quickly reach the global optima in many of these problems which causes it to eventually
be overtaken by the model building techniques.

For all problems, LTGA lags behind P3 in fitness improvement, but has a similar overall shape. On every problem except
Rastrigin, LTGA has a significant early delay in fitness improvement, followed by near instantaneous improvement, plateau,
and followed by a final period of improvement which results in the global optimum. This early period corresponds with
initialization of the population, with the first fitness gain achieved immediately upon completing the first generation.
This makes sense as LTGA's mixing strategy performs a sort of local search when the model has not yet detected gene epistasis.
This brings LTGA's fitness to almost the same level as the techniques explicitly performing local search, but is not quite
as effective at reaching local optima. Subsequent generations then make minor fitness improvements, but generally stagnate
while the model remains uninformed and high quality building blocks in the population are still relatively rare. Once the model
becomes accurate enough and the probability of a crossover using high quality genetic material increases sufficiently, LTGA
enters a second period of rapid improvement.

This behavior is easiest to understand on the two Trap problems. LTGA's first generation will push individual traps toward a
local optima, but it requires more than a single generation to do so completely. Until that occurs, the model is likely behaving
no better than random as there is little apparent linkage between bits. Once the model begins to identify individual traps crossover
can begin to increase the frequency of higher fitness local optima. This process begins slowly as low fitness local optima are more
likely to be chosen for donation than high fitness local optima due to their frequency in the population. The process becomes
self-catalysing as increased frequency of the global optimum trap genes means increased likelihood of the global optimum being spread by crossover.

An almost identical behavior exists in P3, most strikingly in the Deceptive Trap. When solving
Deceptive Trap, P3 immediately achieves a high fitness, equaled only by the Random Restart Hill Climber in quality. This rise is
nothing more than bringing every trap to one of the two local optima, with the majority of traps set suboptimally. $1+(\lambda, \lambda)$
is slightly less efficient about performing single bit improvements, but is the next quickest to reach this plateau. While the hill climber
remains stuck, P3's use of crossover slowly begins to combine optimal trap values found by multiple independent restarts. Unlike LTGA which
performs this improvement all at once, P3 integrates all optimal trap values immediately, waiting only on random restarts to find
the optimal trap setting.

\begin{figure}[t]
  \begin{centering}
    \begin{subfigure}{.5\textwidth}
      \begin{centering}
        \includegraphicsfit{small-pop-dst}
      \end{centering}
      \caption{Deceptive Step Trap 203}
      \label{fig-small-pop-dst}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
      \begin{centering}
        \includegraphicsfit{small-pop-nk}
      \end{centering}
      \caption{Nearest Neighbor NK 200}
      \label{fig-small-pop-nk}
    \end{subfigure}
  \end{centering}
  \caption{Comparison of how reducing LTGA's population size effects
           the median best fitness reached during search.}
  \label{fig-small-pop}
\end{figure}


It is important to note that while P3 generally achieved the best fitness across all time points, hBOA and LTGA were tuned specifically
to find the global optimum and are not necessarily using the optimal population size for finding intermediate fitnesses. Consider
that for both reducing the population size would almost certainly improve their fitness for evaluation steps less than their population
size. To examine the effect of population size we tested LTGA using one tenth of the population size required for reliably finding the
global optimum, with the results given in Figure~\ref{fig-small-pop}. The two problems shown are representative of the behavior of using
a smaller population size on the other five problems. Reducing the population size resulted in LTGA's initial fitness improvement
occurring earlier, which makes sense as the first generation is completed much more quickly. On Deceptive Step Trap this means that
LTGA leaps ahead of P3 due to its ability to overcome the two bit fitness plateaus. Across all tested problems using the smaller
population size reduced the quality of LTGA's first fitness plateau, likely due to missing required diversity to reach higher quality.
The second period of improvement also comes earlier and reaches a lower quality fitness. On Deceptive Step Trap this prevents all 100
runs from reaching the global optimum, down from 100 successful for the full population size. On Nearest Neighbor NK this drops the number
of successful runs from 98 to 68. As a result we conclude that while reducing the population size of LTGA may improve its quality
at early points during optimization, doing so reduces the robustness of the final solution found. This is in contrast to P3
which balances high quality intermediate fitness without trading away eventual optimality.

\section{Computational Expenses}
\begin{figure}[t]
  \begin{centering}
    \begin{subfigure}{.5\textwidth}
      \begin{centering}
        \includegraphicsfit{rebuilds}
      \end{centering}
      \caption{Rebuilds}
      \label{fig-rebuilds}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
      \begin{centering}
        \includegraphicsfit{donations}
      \end{centering}
      \caption{Donations}
      \label{fig-donations}
    \end{subfigure}
  \end{centering}
  \caption{Computation costs incurred by model rebuilding (Figure~\ref{fig-rebuilds}) and
           repeated donations (Figure~\ref{fig-donations}) per evaluation as problem size increases.}
  \label{fig-costs}
\end{figure}

When discussing the asymptotic complexity of P3 in Section~\ref{sec-p3}, two aspects eluded precise
analysis: how expensive is model rebuilding and how many gene donations are made. Figure~\ref{fig-costs}
provides some insight into how often these two aspects of the algorithm are utilized.

Figure~\ref{fig-rebuilds} reports in an algorithmic sense how expensive model rebuilding for search.
In order to calculate this value we recorded how many times search rebuilt the model during each run.
Under the minimal assumption that an evaluation costs $O(N)$ time and that model rebuilds cost $O(N^2)$,
Figure~\ref{fig-rebuilds} shows the ratio cost of model rebuilding per evaluation. If the cost of model building
scaled linearly with evaluations, the relationship plotted for each problem should be asymptotically linear.
For Nearest Neighbor NK, Ising Spin Glasses, and Rastrigin this appears to be the case. For both Trap problems
and HIFF there appears to be slow growth in the ratio. The problem sizes used for MAX-SAT were not sufficient
to accurately gauge the asymptotic behavior. Together this suggests that while the cost of building the model
is almost linear per evaluation, it can grow slowly. However, even in the worst case (HIFF) this growth was
bounded to no more than twice the algorithmic cost of an evaluation.

When applying a crossover subset, P3 tries random donors from the population until one is found with at least
one bit different from the improving solution. In theory this can result in up to $O(\mu)$ operations.
Figure~\ref{fig-donations} examines the observed average number of donations per evaluation performed.
Ising Spin Glass, HIFF, and Rastrigin all achieve effectively constant behavior here, implying repeated
donation does not impact the asymptotic runtime of P3. Both Trap functions and Nearest Neighbor NK all
increase in number of donations as problem size increases, potentially increasing algorithmic costs. An
important note again is that each donation may range in size from a single bit up to $N-1$, with smaller
donations far more likely to result in repeated attempts. As such this may cause some super-linear growth
in P3, but its unlikely to be very high.

\begin{figure}
  \begin{center}
  \includegraphicsfit{seconds-to-success}
  \end{center}
  \caption{Comparison of the median number of seconds to reach the global optimum for
           the six different optimization methods with respect
           to problem size.  If the median run did not reach the global optimum no data element
           is shown.  Results given on a log-log scale.}
  \label{fig-seconds-to-success}
\end{figure}


To assess wall clock performance we provide Figure~\ref{fig-seconds-to-success}. Due to the scale
of our experiments, diverse hardware potentially running multiple processes in parallel were used
in order to collect all runs. As such these timings should be considered very skeptically as a number
of sources may contribute to biased error in the results. Under those caveats, the results do suggest
that any increase in P3's algorithmic complexity is offset by its efficient use of evaluations. On
Deceptive Trap, Nearest Neighbor NK, Ising Spin Glasses, and MAX-SAT it still appears to find the global
optimum faster than all comparison techniques. On HIFF LTGA is able to match P3's performance almost exactly,
and on Rastrigin P3 falls behind only on moderate sized problems. Deceptive Step Trap yields the worst performance,
with LTGA using optimal population sizes having a significantly lower growth curve than P3. This mirrors
the results from Figure~\ref{fig-evals-to-success} under the assumption that P3's computational time per evaluation is higher
than LTGA but by no more than a constant factor.

When considering wall clock time hBOA and Parameter-less hBOA perform significantly worse than when using
evaluations. This makes sense as hBOA's model building requires $\Omega(N^2)$ time per evaluation while under reasonable
assumptions P3 and LTGA require $O(N)$ time per evaluation. This penalty is most clear on Ising Spin Glass where
hBOA goes from being slightly more efficient than LTGA in terms of evaluations to significantly worse in terms of seconds.
As P3 and LTGA require a similar asymptotic complexity per
evaluation as the Hill Climber and $1+(\lambda, \lambda)$, no similar change in ordering occurs.

These results suggest that even for minimally expensive fitness functions P3 can find the global optimum
at least as quickly in seconds as its optimally tuned competitors. However, due to the potential noise in our data collection
we are hesitant to draw too strong of a conclusion.

\section{Inner Workings}

\begin{figure}[t]
  \begin{centering}
    \begin{subfigure}{.5\textwidth}
      \begin{centering}
        \includegraphicsfit{cross}
      \end{centering}
      \caption{Crossover Proportion}
      \label{fig-cross}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
      \begin{centering}
        \includegraphicsfit{cross-success}
      \end{centering}
      \caption{Crossover Success}
      \label{fig-cross-success}
    \end{subfigure}
  \end{centering}
  \caption{For each problem Figure~\ref{fig-cross} shows the proportion of P3 evaluations spend on crossovers
           and Figure~\ref{fig-cross-success} shows the percentage of fitness improving crossover evaluations.}
\end{figure}

While analysis of optimization speed is useful from a practitioner standpoint, doing so provides very
little insight into algorithm behavior. To better understand how P3 works in detail we present here
a look at some internal features specific to P3.

P3 performs optimization using a mixture of hill climbing and crossover. Exactly how much time the algorithm
dedicates to each is not clear from the algorithm description alone. As such, Figure~\ref{fig-cross} shows
that the relationship between the two is strongly problem dependent but relatively independent of problem size.
In general the trend appears to suggest that problems which can be more tightly represented by crossover tend
to require less crossover evaluations. Deceptive Trap and Rastrigin spent the least evaluations on crossover, using
15\% and 20\% respectively on the largest problem sizes. Both of these problems are perfectly separable and have relatively
few local optima in each subproblem. HIFF and Ising Spin Glasses use crossover about 60\%. The HIFF structure can
be perfectly learned by the linkage tree, but the higher order relationships require multiple stages of crossover to optimize.
Ising Spin Glasses have an overlapping structure, making them impossible to perfectly represent in a single linkage tree, but
that overlap is relatively minor in comparison with Nearest Neighbor NK, which uses 81\%. Deceptive Step Trap spends the highest
percentage of evaluations on crossover, 92\%. Unlike all of the other problems, the value taken by an individual bit in Deceptive
Step Trap has very little meaning. This is because any trap value is either a local optima or 1 bit flip away from being a local optima.
This has a combined result of meaning it takes very few local search evaluations to find a local optima but very many crossover operations
to proliferate the global optima. Epistatic relationships are also going to be very weak and even small population sizes are likely
to mark all clusters as $useful$.

Solutions are only added to the next highest pyramid level if at least one crossover donation resulted in a fitness
improvement. Figure~\ref{fig-cross-success} examines the percentage of crossover donations that resulted in a fitness
improvement. The ordering in Figure~\ref{fig-cross-success} is an almost perfect inversion of Figure~\ref{fig-cross},
implying that the more successful crossover is at achieving fitness improving donations the lower the proportion of evaluations P3 will
spend performing crossover. P3 achieves an impressive 30\% success rate for crossover donations on the largest Deceptive Trap
problem sizes. This makes sense as crossover is able to immediately and perfectly identify entire traps such that all donations
always contain only complete traps. Furthermore, by forcing crossover to attempt donations of genes different from the current solution,
donations containing a single trap will have a very high success rate, always improving suboptimal traps which are far more frequently
created by local search that optimal traps. Rastrigin and HIFF have relatively high success rates of 11\% and 7\%, respectively. Again
this is likely due to accurate modeling and relatively low diversity of building block alternatives.
Ising Spin Glasses and Deceptive Step Trap both have low success rates near 0.01\%. This follows as both are difficult to model with
a linkage tree. In comparison to Figure~\ref{fig-cross-success} Deceptive Step Trap performs better than expected for crossover.
However, the reason for this is likely that even exceedingly uninformed crossover can make improvements to Deceptive Step Trap as
flipping two bits in the same trap will move between two local optima. Nearest Neighbor NK had the least successful crossovers,
with only 0.007\% successful on the largest tested problems. In comparison to Ising Spin Glass, NK's overlap is far more extensive.
As a result it makes sense that individual crossovers are going to be very unlikely to be fitness improvements.

In comparison with LTGA, the crossover success rates of P3 are lower but similar in problem ordering. This is almost certainly
a consequence of P3's use of local search. Counter intuitively the use of hill climbing on the initial population is likely reducing
P3's crossover success not because it reduces model quality or the donation pool, but because it is much more challenging to improve
locally optimal solutions than randomly generated ones. LTGA's crossover benefits from application to unoptimized solutions, which
makes its aggregate crossover success incomparable to P3's.

While the frequency of crossover changes per problem, and the likelihood of an individual crossover making a fitness improvement can be
quite low, the results from Section~\ref{sec-optimum} and Section~\ref{sec-overtime} suggest its still critical to optimization. Without
crossover P3's performance would be identical to the Random Restart Hill Climber, which was unable to solve even moderately sized problems
and quickly fell behind P3 in intermediate fitness quality. Therefore even the infrequently successful crossover donation are
critical to success. This does however suggest a potential avenue for future improvement as different modeling and donation algorithms
may improve P3's crossover success.

\begin{figure}[t]
  \begin{centering}
    \begin{subfigure}{.5\textwidth}
      \begin{centering}
        \includegraphicsfit{level-size}
      \end{centering}
      \caption{Population Size}
      \label{fig-level-size}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
      \begin{centering}
        \includegraphicsfit{level-success}
      \end{centering}
      \caption{Crossover Success}
      \label{fig-level-success}
    \end{subfigure}
  \end{centering}
  \caption{For each problem Figure~\ref{fig-level-size} shows the number of solutions stored in each level of the pyramid
           and Figure~\ref{fig-level-success} shows the percentage of fitness improving crossover evaluations at each level.}
\end{figure}

Another feature unique to P3 worth investigating is the shape and size of the population pyramid
constructed for each problem. Figure~\ref{fig-level-size} shows the number of solutions stored at
each level of the pyramid for the largest tested problem size for each problem. Each point is the
median size across 100 runs. If a run did not store any solutions at a level it is treated as 0,
and no point is drawn if the median run had 0 solutions stored at that level. This behavior is
representative pyramid shape across all problem sizes tested.

With the exception of the dip in Deceptive Step Trap, all of the pyramids show a monotonic reduction
in size as the level increases. This makes sense from an algorithmic standpoint as solutions are
added to the first level as long as they are unique local optima. For each subsequent level a
solution must be unique and a strict fitness improvement over its previously added version.
As a result it makes sense that it is generally less likely for a solution to be added to
higher levels than lower ones. This shape is also in many ways desirable from a population
genetics standpoint. When using a generational model \cite{lobo:2011:dynamicpop} found theoretical
evidence that the optimal population size reduces each generation with \cite{goldman:2011:dynamic-parameters}
providing supporting empirical data. In P3, levels of the pyramid are akin to generations as solutions
stored in each level have undergone the same number of variation and selection events,
with higher levels representing more of each. Therefore it is possible that P3's
pyramid shape is related to its ability to outperform competitors using an optimal
fixed population size. Subsequently smaller levels are able to focus search,
preserving only the diversity required by that stage of search. In comparison,
LTGA and hBOA set the population size at each generation equal to that required
to ensure sufficient initial diversity.

The number of solutions stored at the fourth level of Deceptive Step Trap is
significantly lower than that of the third or fifth levels, breaking the decreasing
trend of the other six problems. This behavior is rooted in the peculiar nature
of this landscape, and is also present in all other problem sizes tested.
After local search, all traps in all solutions have a total
number of 1 bits equal to \{0, 1, 3, 5, 7\} as these correspond to the local
optima when using $k=7$ and $s=2$. Crossover is very likely to overcome the
two bit plateaus, and as a result solutions in the second level are unlikely
to contain the lowest fitness local optima of 5 set bits. Similarly the third
level has very few traps set to the next worst local optima of 3 bits. As a result,
most solutions which reach the third level can only be improved by overcoming deception
and replacing the second best local optima of zero or one bits set to having all seven bits set.
At this point in search the population has not significantly increased the frequency of optimal
trap settings and with 8 ways to represent suboptimal local optima linkage is also likely to be
inaccurate. As a result our expectation is that when performing crossover with the third level
of the pyramid very few fitness improvements will actually be made, meaning
few solutions will be added to the fourth level. Those solutions that are added must, by definition,
contain a higher frequency of optimal trap settings to have achieved a fitness improvement. As
a result the crossover success rate and modeling quality at level four is likely to be much better
than at level three, increasing the likelihood of solutions being improved by crossover.

When examining the crossover success rate shown in Figure~\ref{fig-level-success} these suppositions are
supported. The success rate for Deceptive Step Trap decreases for the first three levels, with the third level
successful only 0.0004\% of the time. This quickly rebounds at higher levels. On all other problems, the
crossover success rate follows a trajectory closer expectation. Low levels are more successful than medium
levels as it is easier to make improvements to less optimized solutions. This trend reverses somewhat at
higher levels as solution filtering allows for highly accurate models and high frequencies of high quality
building blocks. The highest level on most problems has a very low crossover success rate, likely because
so few solutions other than the global optimum every cross with that level.

\section{Conclusions and Future Work}

\small

\bibliographystyle{apalike}
\bibliography{../main}


\end{document}
