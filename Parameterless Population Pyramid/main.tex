% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}
%\linespread{2.5}
\usepackage{url}
\usepackage{verbatim}
\newcommand{\includegraphicsfit}[1]
{\includegraphics[width=\columnwidth,height=\textheight,keepaspectratio]{#1}}

\newcommand{\includegraphicswide}[1]
{\includegraphics[width=.9\textwidth,height=\textheight,keepaspectratio]{#1}}

\usepackage{bm}
\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{GECCO'14,} {July 12--16, 2014, Vancouver, BC, Canada.}
\CopyrightYear{2014}
\crdata{978-1-4503-1963-8/13/07}
\clubpenalty=10000
\widowpenalty = 10000
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Parameter-less Population Pyramid}
\subtitle{[Genetic Algorithms Track]}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %
\begin{comment}
\author{
% 1st. author
\alignauthor
Brian W. Goldman\\
       \affaddr{BEACON Center for the Study of Evolution in Action}\\
       \affaddr{Michigan State University, U.S.A.}\\
       \email{brianwgoldman@acm.org}
% 2nd. author
\alignauthor
William F. Punch\\
       \affaddr{BEACON Center for the Study of Evolution in Action}\\
       \affaddr{Michigan State University, U.S.A.}\\
       \email{punch@msu.edu}
}
%\end{comment}
%\begin{comment}
\author{
% 1st. author
\alignauthor
Anonymous\\
       \affaddr{Group}\\
       \affaddr{Organization}\\
       \email{email@site.com}
% 2nd. author
\alignauthor
Anonymous\\
       \affaddr{Group}\\
       \affaddr{Organization}\\
       \email{email@site.com}
}
%\end{comment}

\maketitle
\begin{abstract}
%Maximum 200 Words
  Real world applications of evolutionary techniques are often hindered by
  the need to determine problem specific parameter settings.
  While some previous methods have reduced or removed
  the need for parameter tuning, many do so
  by trading peak efficiency for general applicability.  The
  Parameter-less Population Pyramid (P3) is an evolutionary technique
  requiring \textbf{no} parameters which is still broadly applicable to
  many problem types. P3 strikes a balance between continuous
  integration of diversity with exploitative elitist operators,
  allowing it to solve easy problems quickly and hard problems
  eventually.  When compared with three state of the art optimization
  techniques across seven different problem classes, P3 always finds
  the optimum at least a constant factor \emph{\textbf{faster}} than
  the rest.  Even when optimally tuning comparison technique
  parameters, P3 has a lower evaluations to success growth complexity when test problems come
  from a random distribution.  Beyond empirical results, we suggest
  avenues for theoretical analysis of P3's expected runtime.  In
  general, while these are only the first results for P3, we feel they
  indicate wide applicability.
\end{abstract}

% A category with the (minimum) three required fields
%\category{Computing Methodologies}{Artificial Intelligence}{Search Methodologies}
\category{TODO}{TODO}{TODO}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{Algorithms}

\keywords{Linkage Learning, Local Search, Parameter-less}

\section{Introduction}
Providing a good solution in reasonable time to a broad class of real
world black box problems is a fundamental goal of evolutionary
computation.  Achieving this goal often requires an optimizer to make
assumptions about the nature of the problem it is solving as no
optimizer can solve all problems equally well~\cite{Wolpert:1997:nfl}.

Often built into the design of a genetic algorithm (GA) are
evolutionary parameters which allow the user to tune the optimizer's
assumptions to match a specific problem.  These parameters can allow
a GA to more efficiently search the problem's landscape.  However, to
do so the user must first determine how to correctly \textbf{tune} the
parameters, typically a complex and time consuming process unto
itself.

Some considerable work has been devoted to understanding how to tune
these parameters. Early work in Evolutionary Strategies introduced the
concept of self-adaptive evolution which was subsequently introduced
to genetic algorithms~\cite{Back:1992:selfadapt}. Here, each solution
carries some of the parameters used and those parameters are adapted
along with the solution they are co-encoded with. Other work introduced the concept
of a \emph{meta} algorithm~\cite{grefenstette:1986:optimalga}. Here a
second optimizer runs ``on top'' of the actual problem-solving
optimizer, the second slowly tuning the parameters of the first. Both
of these approaches are rather slow, in particular the meta
approach, as actual problem solving must occur at set levels to give
feedback as to how to change the parameters (online or to the meta
optimizer). 

Many parameters have ``reasonable'' settings that are
unlikely to effect the order of complexity of a genetic algorithm,
with population size being a notable exception. For example, an overly small
population can cause exponential search times, while arbitrarily large
population sizes can lead to wasted evaluations and stalled search.
More importantly, setting the population size correctly relies on
difficult to discern features of each problem. 
Goldberg~\cite{goldberg:1991:gasize} showed that small
population sizes were not capable of finding good solutions, though
some practitioners~\cite{haupt:2000:optimum} have found that small
populations with a large mutation rate are effective on some practical
problems.

These complications have encouraged work to remove the need for
operator defined evolutionary parameters. In
an early example, a crossover based GA was devised which used
concurrent racing populations to remove all user specified
parameters~\cite{harik:1999:parameterlessga}.  This method of
population racing has also been paired with more advanced evolutionary
system, such as Hierarchical Bayesian
Optimization~\cite{pelikan:2004:parameterlesshboa}. The claim was that
while not optimal, this method ensured the GA would take no more than
a logarithmic factor increase in evaluations beyond optimal.  Furthermore, doing so would
ensure that these optimizers could be generally applicable without
problem specific tuning.  Experimental evidence supports this theory,
with some evidence that in general the bound is closer to a constant
factor worse than optimal.

In this work we shall introduce the Parameter-less Population Pyramid
(P3).  P3 is an iterative method for constructing a \textbf{collection} of
populations that requires no user parameters.  It leverages entropy based
linkage detection to learn how to efficiently mix solutions without
problem specific information beyond the evaluation function.  Perhaps
most importantly, unlike other parameter-less techniques it
experimentally appears to be at least a constant factor
\emph{\textbf{improvement}} over comparable optimally configured
optimization methods.

\section{The P3 Algorithm}
\label{sec-p3}
Before going into detail about how each aspect of P3 works, let us
consider an example of P3 operation. Unlike many EC techniques, P3
does not have a single population of solutions.  Instead P3 maintains
a pyramid like structure, such that each level of the pyramid has a
population of solutions, with each level representing something akin
to different generations of evolution. Consider the example evolution
presented in Figure~\ref{fig-p3}.  At the start of optimization P3
has no information about the landscape so solution \#1 is randomly generated.  Solution \#1
is brought to a local optima using a hill climber
(Section~\ref{sec-hillclimber}) creating solution \#2, which is added
to the population pyramid (Section~\ref{sec-pyramid}) at population
level $P_0$.  With a population of 1 at $P_0$, there is insufficient information to continue,
so solution \#3 is randomly generated and optimized via hill climbing
to create solution \#4, which is likewise added to the population
pyrimad at population level $P_0$.  With a population of two,
crossover (Section~\ref{sec-crossover}) is applied to solutions \#2
and \#4 creating solution \#5, which is an improvement. As \#5 used
information from $P_0$ it is added to the next level of the pyramid at
population level $P_1$.  Because of the crossover technique that P3
uses, there is no more information to be gained from the present
population. Thus, \#6 is randomly
generated and improved into \#7.  Unlike \#2 and \#4, \#7 is not added
in this example because it happens to be identical to a solution
already in the pyramid.  Even though \#7 is not unique, it still
crosses with $P_0$, which does make a unique improvement \#8 that is
added to $P_1$ and mixed with that level to create \#9.  As before,
the process creates \#10, \#11, and \#12 using random generation, hill
climbing, and crossover, respectively.  In this example \#13 is not
added to $P_2$ because it is not a strict fitness improvement over
\#12.  There is not enough information in $P_2$ to improve \#13
further, so the process would begin yet again with a random solution.

\begin{figure}
  \centering
  \includegraphicsfit{P3_big_fail}
  \caption{Example of the first steps of P3's optimization described in Section~\ref{sec-p3}.
  \#7 is not added because it is a duplicate of a solution already in the pyramid.  \#13 is
  not added because it is not a fitness improvement over \#12.}
  \label{fig-p3}
\end{figure}

\subsection{Hill Climber}
\label{sec-hillclimber}
% The terminology used to describe different methods of making minor modifications
% to a solution, evaluating the quality of those changes, and iteratively constructing
% new solutions, is full of duplicated, ambiguously defined terms.  As such we shall
% describe what is likely an existing algorithm and give it a descriptive name that
% may not match all of its previous incarnations.

For our purposes, we use a First Improvement Hill Climber
(FIHC)\footnote{We use the name FHIC for an algorithm that likely has
  various names in the literature, but use it here with a descriptive
  name for clarity.}, a method of exploiting local information through
a series of modify-evaluate-accept iterations starting from a randomly
initialized solution and ending with an optimum, either local or global.
FIHC uses the following algorithm.  Consider all gene loci
$x_i$ in a random order.  For a given $x_i$ try all possible
alternative values for the gene $x_i$, evaluating solution quality
with each change.  If any alternative value is a strict fitness improvement
over the current value for $x_i$, replace $x_i$ with the improved
value.  Iteration of the process continues until no such change results in
a strict fitness improvement.

To avoid wasting evaluations, FIHC keeps track of all loci tested since the last
change was made to the solution.  If the algorithm returns to the same loci without
any changes, it moves on to the next loci without performing any evaluations.

FIHC solves all functions of unitation in $O(N)$ time.  This is in contrast to
methods like Steepest Ascent Hill Climber~\cite{bosman:2011:lsbbo, goldman:2012:ltga}
which require $O(N^2)$ as changes are only accepted after all neighbor solutions
are evaluated.  In general, FIHC is designed to find optima using a minimum number
of evaluations.  Furthermore, because FIHC tests loci in a random order it does
not cause any positional bias to the search.


\subsection{Pyramid}
\label{sec-pyramid}
As mentioned, P3 maintains a pyramid like structure of populations
with each level representing something akin to different generations
of evolution. More specifically, a solution's level is one higher than
its highest parent, similar to the layering done
in~\cite{hornby:2006:alps}.

The levels represent independent sets of unique solutions, such that
$\forall_{i, j \in P, i \neq j} P_i \cap P_j = \emptyset$.  This relationship
is maintained constructively, such that new solutions can only be added to a level
if they do not exist in any level of the pyramid.  This uniqueness check can be
achieved in constant time by maintaining a hash of all solutions in the pyramid.

Along with storing the solutions, each level also maintains a table of pairwise
gene value frequencies.  This is also built constructively, such that initially
all frequencies are set to 0.  Each time a solution is added to a level, all
pairs of loci are queried for their gene values.  The table is then updated,
incrementing the count for each observed pairing.  This requires $O(N^2)$ operations
(no evaluations) each time a solution is added to a level of the pyramid, but is
constant in the number of solutions currently in the level.

\subsection{Crossover}
\label{sec-crossover}
P3 uses a crossover method derived from that defined for LTGA~\cite{thierens:2013:ltgahiff}
(Section~\ref{sec-ltga});
P3 uses gene value entropy to construct clusters of genes that should have their values
conserved during crossover.  There are two primary steps in
defining this type of crossover: \emph{cluster creation} and \emph{cluster usage}.

During cluster creation, P3 attempts to create a binary tree of clusters such that the leaves
represent individual gene loci and each internal node represent the subset of loci created
by joining the subsets encoded in its children.  These clusters represent linked
loci that have been detected in the genome, and are used when performing crossover.
Using the table of observed frequencies described in Section~\ref{sec-pyramid}, Equation~\ref{eq-distance}
efficiently estimates the linkage between clusters of loci using the average pairwise
entropy, as given in Equation~\ref{eq-entropy}.
\begin{equation}
  D(C_i,C_j) = \frac{1}{\left | C_i \right |\cdot \left |C_j \right|}\sum_{c_i \in C_i}\sum_{c_j \in C_j} 
  2 - \frac{H(C_i) + H(C_j)}{H(C_i \cup C_j)}
  \label{eq-distance}
\end{equation}
\begin{equation}
  H(C) = -\sum_{s\in S} p_c(s)\log(p_c(s))
  \label{eq-entropy}
\end{equation}

Starting from the leaves, the binary tree can be constructed using an
agglomerative method.  Each
iteration determines the pairwise linkage between each cluster of loci
using Equation~\ref{eq-distance}, and merges the two closest into a new
cluster.  The process ends when there is only a single cluster
containing all loci.  As our clustering algorithm is both convex and
satisfies the swapping lemma~\cite{gronau:2007:upgma}, this algorithm
requires $O(N^2)$ total time to correctly construct the linkage tree
from the frequency table.

Not all of  the resulting clusters in the linkage  tree are useful for
crossover.  The most obvious useless  cluster is one that contains all
the loci.   A crossover that preserves  all loci in  a solution cannot
create  a  new  individual.   We  further  remove  clusters  that  are
completely  contained   within  some  other  cluster,   such  that  if
$D(C_i,C_j)  =  0$, $C_i$  and  $C_j$  are  not used  when  performing
crossover.   $D(C_i,C_j)$ only  returns  $0$ when  the  loci in  $C_i$
perfectly predict the gene values in $C_j$ and vice versa. From this we can
conclude  that  the  settings  for  those two  loci  should  never  be
disrupted.  When  used on particularly small populations  this has the
potential  to  greatly  reduce  the  number of  clusters  used  during
crossover.  For  instance, if  $|P_i| = 1$,  no clusters exist. If
$|P_i| =  2$ at most two clusters  exist: one for all  genes where the
solutions agree and one for where they disagree.

Given a useful set of clusters, we can now define how they are used.
P3 uses crossover to recombine known information in a level of the
pyramid with a candidate solution.  The algorithm is as follows. For
each cluster select a random donor from the population.  Copy the
donor's gene values for all loci in the cluster into the candidate
solution.  Evaluate the candidate.  If the fitness of the candidate
has been reduced, revert the candidate's gene values.  It is important
to note that candidate changes are kept in the case when the fitness
is \emph{unaffected}.  This allows crossover to drift, potentially
moving through plateaus.

When choosing a donor, P3 ensures there is at least one gene value
different between the donor and the candidate solution.  This can
potentially require up to $O(|C_j||P_i|)$, but empirically appears
more efficient than allowing donations that do not change any gene
values.

To complete a single crossover event, all of the useful clusters are tested, each
requiring at most a single evaluation.  Clusters are tested in smallest first order,
with clusters of equal size ordered randomly.  This ensures smaller building blocks
are optimized before large changes are made to the genome, thus helping preserve
diversity.  This also helps prevent genetic hitchhikers, which may interfere with
entropy calculations.  As cluster building and application ordering are not dependent on
the position of loci in the genome, this operator introduces no positional bias in the search points
evaluated.

\subsection{All Together}
\label{sec-alltogether}
The pieces of P3 were specifically chosen to provide important complimentary capabilities.  First,
the hill climber provides an efficient tool for discovering pairwise linkage, as
search only ends when no single bit change can be made which improves fitness.
Therefore crossover, which relies on pairwise linkage, has high quality information
to leverage when determining clusters.

Similarly, by starting from a random position for each new solution, P3 progressively reduces
the chance of spurious linkage formation due to initialization.  Furthermore, these random
restarts provide exploration to balance out the hill climber and crossover's extreme
exploitation.  This also allows the population to grow large enough to contain enough
diversity to solve the problem without having to know the optimal size beforehand.

Finally, the use of levels allows P3 to perform solution filtering
without throwing away information.  Once found, a local optima is
stored indefinitely, preserving both its linkage and genes for future
crossovers.  By splitting solutions into levels, the linkage
information and the donated genes come from similar solution
qualities.  This prevents noise in gene linkage which only exists in
low quality solutions from effecting cluster formation for high
quality solutions.  It also reduces the chance for high quality
solutions to simply overwrite new, unoptimized solutions, preserving
search diversity.

  A major advantage to the design of P3 is that it makes very weak
  assumptions about the search landscape.  In many evolutionary
  systems the crossover operator assumes that genes linked in the
  search landscape are collocated in the genome.  This introduces an
  initialization problem, changing the difficulty of the problem due
  to a, potentially unknown, ordering required on the genome.  All of
  P3's operations ignore gene ordering, removing that level of
  configuration from the user.

\subsection{Contrasted With The Generational Model}
A major advantage of P3's structure is that it avoids the need to set
a fixed population size.  In a generational model, the diversity
contained in the initial population represents a bottleneck for future
generations.  This is true because in most systems, especially on
deceptive problems, mutation rates are low enough as to make the
creation of large, multi-gene substructures unlikely after
initialization.  As such the generational model often results in a
race between decreasing diversity and increasing fitness. This means the
population size must be set large enough to contain sufficient initial
diversity and to maintain that diversity during search for just long
enough for all of it to be leveraged.

Premature convergence of a population leads to the generational model
often having two termination conditions: out of time or out of
diversity.  In the former, if a smaller population size had been used,
more of the diversity could have been exploited meaning a better
solution likely could have been reached in the same number of
evaluations.  In the latter, if a larger population size had been
used, more evaluations could have been performed increasing the
likelihood of improving solution quality.  In
either case, unless the population size is correctly set, the
generational model will likely result in waste.

As P3 has exploited all known information each time the top of the pyramid is reached,
no evaluations are wasted  on unused diversity.  Similarly, because P3
does not throw away previously optimized solutions while adding diversity, it avoids
waste caused by premature convergence. It is this essential balance of
exploitation to diversity that P3 focuses on.

In general, P3 only makes two assumptions: local structure and non uniform gene interdependence.
In order for P3 to be efficient there needs to be at least some correlation between a
solution's fitness and the fitness of its single bit neighborhood.  This is required for
the hill climber to function properly.  Under this assumption, each time the hill climber
improves a solution it also improves the expected fitness of the neighborhood of the solution.
Stated another way, this assumes that small changes in genotype result in small changes in phenotype.
The non uniform gene interdependence assumption is required by the crossover operator to
perform clustering.  Without this assumption, the clusters formed by crossover will
detect spurious relations. The resulting donation of genes between solutions will be
no better than random modification.  Both assumptions are likely to hold true for
real world problems and are similarly assumed by most generational models.

\subsection{Order of Complexity}
With the exception of crossover, P3 requires no more than $O(N)$ computations
per evaluation, implying it is limited by the time required to evaluate a solution.
In order to perform crossover, P3 must spend $O(N^2)$ time updating pairwise
frequencies each time a solution is added to a level of the pyramid.  Similarly,
it requires $O(N^2)$ time to rebuild the set of crossover clusters once the frequency
table has been updated.  Na\"ively, this leads to the algorithm itself requiring $O(N^2)$ time.
However, multiple evaluations can occur for each time a single individual is added
to a population.

Each time a random restart occurs, at minimum $N+1$ evaluations are performed.
This means each time a solution is added to $P_0$, at least $O(N)$ evaluations
must be performed.  This implies that the $N^2$ cost of the update to $P_0$ can
be amortized over these evaluations, making this update $O(N)$ per evaluation.  Note
that $N+1$ is a very weak lower bound, as this only occurs if the randomly generated
individual is already at a local optimum.  Furthermore, only restarts that result
in a unique individual cause updates to $P_0$.

To cause an update to $P_i$, all of the crossovers discovered for
$P_{i-1}$ must first be applied.  Each causes their own evaluation.
If P3 were to na\"ively use all crossovers and perform an evaluation
each time, this would cause $O(N)$ evaluations.  Again, this would
result in $O(N)$ amortized cost per update to $P_i$.  However,
Section~\ref{sec-crossover} discussed how P3 will ignore some clusters
and will also skip an evaluation if no change to the solution was made
by a crossover.  As such we are currently unable to state conclusively
that updating $P_i$ will require $O(N)$ evaluations.  That said,
empirically it appears to be true.  Furthermore, it seems counterintuitive
that intentionally wasting evaluations would be considered beneficial
to runtime.

There is one more operation in P3 that may not scale linearly with
genome size per evaluation: finding a non-identical donor.  When
performing crossover P3 iteratively searches a population for a random
donor which contains genes which differ from the current solution for
the current crossover cluster.  In the worst case, this operation
could require $|C_j||P_i|$, where $|C_j|$ is the number of genes in
the crossover and $|P_i|$ is the number of solutions in the
population.  However, $|P_i|$ grows very slowly.  Also, in empirical
evaluation, P3's wall clock time per evaluation appears to scale
linearly with problem size.

%TODO Consider discussing memory usage.


\section{Comparison Algorithms}

\subsection{Random Restart First Improvement Hill Climber}
%TODO Need a citation either here or Section~\ref{sec-hillclimber}.

As P3 utilizes a hill climber to perform optimization, it is necessary
to show P3's added complexity is able to outperform an optimizer that
uses a hill climber alone.  Therefore as the first comparison we
define a complete optimization method based on the First Improvement
Hill Climber defined in Section~\ref{sec-hillclimber}.  This
definition is extended to match P3's style of restarting. Whenever the
hill climber obtains a solution that cannot be improved using a single
bit flip, a new solution is randomly generated and optimized.

If P3's quality relies solely on starting at random points and
applying FIHC, this comparison algorithm should perform even better
than P3.  Like P3, the FHIC-alone algorithm is parameter-less, makes no
assumptions about gene ordering, and will find the global optima given
a sufficient (potentially exponential) number of evaluations.

\subsection{Linkage Tree Genetic Algorithm}
\label{sec-ltga}
The closest relative to P3 in existing literature is the Linkage Tree Genetic Algorithm
(LTGA).  There have been a number of different variations proposed to LTGA since
its original publication~\cite{thierens:2010:ltga}, so for comparison we have chosen
to use the variant described in the most recent publication~\cite{thierens:2013:ltgahiff}.
This algorithm is also the current state of the art, outperforming other algorithms
in black box optimization across numerous benchmarks.

LTGA works by iteratively improving a population of solutions in a generational manner.
Each generation, half of the solutions (chosen using binary tournament selection)
are used to construct a linkage tree just as described in Section~\ref{sec-crossover}.
Each solution in the population is then crossed with the entire population using that
linkage tree, with the results put into the next generation.  This process continues
until the population fails to change between generations, signaling convergence.

Unlike P3, this variant of LTGA does not make use of a hill climber,
and there is evidence that doing so results in decreased
efficiency~\cite{bosman:2011:lsbbo}.  LTGA applies clusters in least
linked first order whereas P3 uses smallest first.  Also, LTGA does
not search for donors until one containing different genes is found.
Instead, if the donated genes are identical, it simply skips the
evaluation.  LTGA requires a population size parameter, maintains a
fixed population size across generations, and does not prevent
duplicate individuals in a population.  LTGA does not add new genetic
information after initialization, meaning that if required diversity
is not contained in the original population or lost in future
generations, LTGA will fail to find the global optimum.

Since LTGA requires a population size parameter, we thought it most fair to
compare against LTGA with an optimized population size, even though P3
receives no such tuning. To set the population size parameter, we use
the bisection method~\cite{goldman:2012:ltga}, which determines the minimum population
size required to meet a specified success criteria.  As P3 will run until the global optima
is found or the evaluation limit is reached, we want to ensure LTGA has a similar requirement
on its success.  Therefore, using the Rule of Three~\cite{jovanovic:1997:ruleofthree}, we
can provide a bound on the probability of failure $\frac{3}{k+1}$ with $95\%$ confidence,
where $k$ is a number of runs without a failure.  Here we state that a population size
is successful if it performs 100 runs without failing to find the global optimum, meaning
the probability of that population failing to solve problems of the same class is bounded above by
$3\%$.  Note that these 100 runs are considered training, and the results from the bisection
runs are not included during the comparative testing.  Furthermore, when testing on problem
classes, bisection is performed on a different set of instances than testing.

Other variants of LTGA have more closely resembled P3~\cite{goldman:2012:ltga},
but as those variants are not in active use, we did not use them for comparison.
However, no LTGA variant has removed the population size parameter
or fully addressed the problem of premature convergence.

There have been other model based optimization techniques that use little to
no parameters.  Specifically LO-LIMD~\cite{posik:2011:parameterless} uses a
population size of one and a parameter free algorithm to perform optimization.
It relies on the existence of completely separable subproblems and noise free evaluation in order to determine
if two genes are linked through perturbation of their values.  While effective on
this class of problems, most real world problems do not fall into this category.
%In general, LTGA is the most powerful general optimizer with few parameters.

\subsection{\bm{$(1+(\lambda,\lambda))$}}
As a final comparison we chose the $(1+(\lambda,\lambda))$ algorithm~\cite{doerr:2013:lambdalambda}
which is currently the best, theory supported, crossover method.  Unlike P3 and LTGA,
$(1+(\lambda,\lambda))$ does not use a population of solutions or any model building
to determine problem variable linkage.  Instead it uses mutation to produce $\lambda$
offspring, selecting the best to recombine with the original parent to produce $\lambda$
more offspring using uniform crossover.  In the original work, a method for controlling
the value of $\lambda$ is provided based on offspring success, meaning there are no
parameters to be set in this algorithm.

In order to make $(1+(\lambda,\lambda))$ a viable method of optimizing problems
with multiple local optima, we modified the original algorithm.  Primarily, if
during search $\lambda \ge N$, search is restarted from a random individual with
$\lambda=1$.  This is done because when $\lambda \ge N$ the mutation rate is greater
than or equal to $100\%$, nullifying search.  Furthermore, this point is only reached
when the algorithm has stalled for a significant number of generations.

We also made minor modifications to selection and to prevent wasting evaluations.
If the best offspring produced in a generation is a mutant offspring, select that
over any of those produced using crossover.  If there is a fitness tie between the
best offspring in a generation, select the one with the maximum hamming distance
to the parent.  This encourages drift over plateaus.  While the original paper
discusses a ``mod'' version of the $\lambda$ control strategy for how to handle
when offspring of equal fitness are produced, we found this conflicted with our
method of restarting.  As such we use the original control strategy, where $\lambda$
is increased if the best offspring in a generation is not strictly better than its parent.
If an offspring produced by crossover has identical genes to either of its parents,
it is not evaluated.

\section{Test Problems}
All four of our optimization techniques share some interesting properties for
problem equivalence.  First, all are fitness scale invariant, in that changing
the fitness values without changing the relation between solution fitnesses does
not effect any of their search methods.  This is true because none of the methods
uses the fitness values directly, only when comparing the relative fitness between
two solutions using boolean operators.  This does not hold true for some optimization
methods, for instance fitness proportional selection.

None of the methods have a bias toward creating specific gene values outside
of the bias created by the problem itself.  This means all of the optimizers
will have unchanged behavior if a problem's inputs are reflected or XOR'd with a fixed string.  As
an example, the problem of \emph{maximize the number of ones} in a solution will be treated identically
to the problem of \emph{maximize the number of zeros} in a solution.  Both are also identical
to the problem of minimizing the hamming distance to any arbitrary solution.

All of the methods are gene order independent.  This means that the location
of genes in the genome can be randomly shuffled without effecting search behavior.
This is not true in many evolutionary systems due to crossover.  For example,
when using one point crossover the distance between genes in the genome effects the
probability they will be kept together through a crossover.

Combined, these features mean that tests on apparently simple problems can be
indicative of search behavior on a large class of actually complex problems.

In choosing test problems, we decided to use only benchmarks with knowable global
optima.  This allows us to measure how many evaluations each method requires to converge to the
global optima, as opposed to just the highest fitness reached.  This is advantageous
as three of the four algorithms have no hard termination condition.  The remaining
algorithm, LTGA, uses a population parameter which can be increased to slow
convergence while simultaneously increasing the fitness reachable before convergence.
Therefore it is difficult to say how and when to declare any of the techniques
converged unless the global optimum is found.

In practice it is often more important to find a good solution quickly than find
the best solution after an exceedingly high number of evaluations.  However, even
a cursory consideration of P3's operators should convince one of its ability to
find high quality solutions in very few evaluations.  This is specifically true of the iterative
application of the first improvement hill climber and the exceptionally elitist
crossover operator.  Therefore we feel it is more important to test P3's resistance
to premature convergence, as that seems a more likely flaw than being slow to reach
``good enough'' solutions.

\subsection{Single Instance Problems}
Well defined test problem landscapes can help when performing exact analysis of
algorithms.  These problems can be thought of as testing the edge cases of optimizers,
presenting them with unnatural but interesting problem features.

The classic example for this kind of test problem is the Deceptive Trap problem.
For this benchmark the genome is broken up into non-overlapping problems of $k$
genes, with the fitness of a single ``trap'' given by Equation~\ref{eq-deceptivetrap}.
\begin{equation}
   trap(t) = \left\{
     \begin{array}{rl}
       k-1-t, &  t<k\\
       k,   &  t = k
     \end{array}
   \right.
  \label{eq-deceptivetrap}
\end{equation}
In this equation $t$ represents the number of bits set in the trap.  For each trap
there is one local optima (all bits set to 0) and one global optima (all bits set to 1).
We chose to use a trap size of $k=7$.
This problem requires exponential time to solve unless there is some method to effectively
move complete traps between solutions, making this an excellent crossover test problem.

However, when using a hill climber and linkage learning, the Deceptive Trap problem
may be trivially solved~\cite{goldman:2012:ltga}.  Thus a more
difficult problem,  Deceptive Step Trap
problem was defined, and is given in Equation~\ref{eq-deceptivesteptrap}.
\begin{equation}
   step\_trap(t) = \left \lfloor \frac{(k-s)\pmod{s} + trap(t)}{s} \right \rfloor
  \label{eq-deceptivesteptrap}
\end{equation}
This problem leverages the original Deceptive Trap problem definition, with the
addition of fitness plateaus of configurable size $s$.  This creates an enormous
number of local optima and makes detection of linkages much more challenging.  We
chose to use a trap size of $k=7$ with plateaus of size $s=2$.

While both of these problems have difficult to solve subproblems, the non-overlapping
nature of the solutions and their single optimal setting may not capture aspects of real world problem solving.
Therefore the Hierarchical If and only If (HIFF) problem is often used to test an
algorithm's ability to handle these characteristics~\cite{thierens:2013:ltgahiff}.
In this problem the genome is broken up hierarchically into a complete binary tree,
with each gene representing a leaf, and each internal node representing the union
between all genes in that node's subtree.  A node in the tree contributes to the fitness
only if all of the genes in its subset have the same value (IE are all 0 or are all 1).
The amount contributed is equal to the number of bits in the subset.  As a result
this problem involves creating larger and larger blocks of single value runs which
only score if the entire block is optimized.

Our final single instance problem is borrowed from the real valued optimization domain.
The Discretized Rastrigin problem uses the standard Rastrigin evaluation function (Equation~\ref{eq-rast}).
\begin{equation}
  An + \sum_{i=1}^{n}\left [ x_i^2-A\cos (2\pi x_i) \right ] \forall x\in [-5.12,5.12]
  \label{eq-rast}
\end{equation}
This function creates a highly multimodal landscape which is completely separable
and has a single global optima.  In order to apply our algorithms to this complex problem,
we discretized the representation using 10 bit gray coded numbers evenly distributed
on the range $[-5.12,5.12)$.

\subsection{Randomly Generated Problem Classes}
By enforcing only certain characteristics of a search space while allowing others
to be generated randomly, we can perform tests on classes of problems instead
of specific instances.  This can improve the generality of results as experiments
then predict the expected quality over the entire class even if only a subset
of possible instances are actually used during experimentation.  Unlike single
instance problems, the challenge with randomly generated problems is often how
to know when the global optimum is reached.

NK landscapes define a class of problems such that the fitness of a single gene's
value is dependent on $k$ other genes in the genome.  Finding the solution which
maximized fitness on an arbitrary NK landscape requires exponential time, and
are therefore a good candidate for heuristic techniques.  Nearest Neighbor NK
landscapes are a subset of NK landscapes such that each gene is dependent on the $k$
genes following it in the genome.  The advantage of this subset is that they can be
solved in polynomial time~\cite{wright:2000:solvingnk}.  For our experiments we
used wrapping neighborhoods, such that genes near the end of the genome depend on
genes at the start of the genome.  We chose to set $k=5$.

Ising Spin Glasses are another large class of $NP-Hard$ problems.  These problems
involve minimizing the interaction terms in a graph based on edge weights by changing
the sign of graph nodes.  Similar to with NK landscapes, there is a polynomially solvable subset of Ising problems
known as $2D\pm J$ Ising Spin Glasses.  This subset restricts graph interactions to a
toroidal two dimensional grid with edge weights of $\{-1, +1\}$.  This subset can
be solved in polynomial time~\cite{saul:1994:spinglass}, and there are existing
tools for solving arbitrary instances.\footnote{\url{http://www.informatik.uni-koeln.de/spinglass/}}

Our final problem class is a subset of the Maximum Satisfiability (MAX-SAT) problem.
This problem consists of a series of disjunctive clauses, such that each clause contains
three problem variables, some of which are negated. While
MAX-SAT requires exponential time to solve, we have devised a method of generating
instances that ensures each has at least one solution that satisfies all clauses.
First, a random target solution is generated.  For each clause, set the sign of each
variable in the clause such that that clause will be satisfied by the target solution.
To ensure there is no bias to variable signs in the generated problem there is a $\frac{1}{6}$ probability
all clause signs match the target, a $\frac{1}{6}$ probability two of the three signs match,
and a $\frac{4}{6}$ probability only one sign matches.  By setting the clause signs
in this way we ensure the target solution is always able to satisfy all clauses,
meaning all problems have a known best fitness.  Note that its possible there to be
other solutions that also satisfy all clauses.  We chose a clause to variable ratio
of $r=4.27$.

\section{Experimental Results}
\begin{figure*}
  \centering
  \includegraphicswide{WithRast}
  \caption{Comparison of the median number of evaluations to reach the global optimum for
           the four different optimization methods on seven benchmark problems with respect
           to problem size.  If the median run did not reach the global optimum no point
           is drawn.  Results given on a log-log scale.}
  \label{fig-results}
\end{figure*}

In total, our experiments performed 100 runs of each optimization algorithm across
7 benchmark problems, with each run limited to 100 million evaluations.
In performing data collection, we performed over 150,000 runs, totaling approximately
1.5 trillion evaluations.  Of these, approximately 100,000 runs were used to optimally
set the population size of LTGA.  Note that these runs should be considered a training phase,
and as such all reported results for LTGA use independent runs to create a testing phase.
This was the only algorithm tested that received such tuning.  As such, LTGA should have a significant
advantage over the others in that it has pre-learned the correct population size.
All of the code used to perform our experiments
as well as individual run results and analysis scripts are available from our
website.\footnote{\url{https://name.of.site.removed/anonymous}}  The results
are summarized in Figure~\ref{fig-results} and Table~\ref{table-results}.

On all 7 problems, the First Improvement Hill Climber and the $(1+(\lambda,\lambda))$
optimizers appear to have a higher growth complexity than P3.  On all of the problems
except MAX-SAT they are also worse than LTGA.  This makes sense as all of the
benchmarks are deceptive in some sense, and neither of these techniques is designed
to handle deceptive landscapes.  The inability for the iterative hill climber to solve
any of the problems indicates that P3 must be doing something more than just local search.

Figure~\ref{fig-results} does provide one misleading
exception for $(1+(\lambda,\lambda))$ on the deceptive trap problem, where it appears to
level out at high genome sizes.  This is entirely an artifact of the extremely high
variance of this algorithm on this problem.  Note that many of the large problem
sizes failed to find the optimum for the majority of runs (no dot drawn for that size).

The behavior of P3 and LTGA on the four single instance problems appears very similar,
with P3 generally being lower by a constant factor.  This makes sense as LTGA's
tuning allows it to determine a single population size sufficient to solve that
instance.  However, because it uses a fixed population size, it must set the that
size large enough to ensure all random initializations of that population are
likely to contain enough diversity to reach the global optimum.  By comparison, P3 is able to
iteratively increase the population size, stopping once enough diversity has been generated.
As such we would expect P3 to do better, but not a lot better.

\begin{table}
	\centering
	\begin{tabular}{|c|r|c|}
	  \hline
	                & \textbf{Median} & \textbf{Confidence Interval} \\ \hline
    \multicolumn{3}{|c|}{\textbf{Deceptive Trap}, N=805, Speedup=2.936} \\ \hline
\textbf{LTGA} & 1,807,127 & 1,805,039 .. 1,808,718 \\ \hline
\textbf{P3} & 615,503 & 577,000 .. 653,504 \\ \hline\hline
    \multicolumn{3}{|c|}{\textbf{Deceptive Step Trap}, N=805, Speedup=1.107} \\ \hline
\textbf{LTGA} & 14,106,751 & 14,102,987 .. 14,109,910 \\ \hline
\textbf{P3} & 12,742,411 & 11,951,148 .. 13,777,090 \\ \hline\hline
    \multicolumn{3}{|c|}{\textbf{HIFF}, N=2048, Speedup=3.1278} \\ \hline
\textbf{LTGA} & 1,847,077 & 1,845,662 .. 1,848,951 \\ \hline
\textbf{P3} & 590,519 & 570,739 .. 608,160 \\ \hline\hline
    \multicolumn{3}{|c|}{\textbf{Discretized Rastrigin}, N=800, Speedup=1.1854} \\ \hline
\textbf{LTGA} & 244,086 & 242,815 .. 245,189 \\ \hline
\textbf{P3} & 205,908 & 201,012 .. 210,913 \\ \hline\hline
    \multicolumn{3}{|c|}{\textbf{Nearest Neighbor NK}, N=600, Speedup=4.408} \\ \hline
\textbf{LTGA} & 37,620,027 & 37,244,617 .. 37,990,772 \\ \hline
\textbf{P3} & 8,533,322 & 7,315,275 .. 9,481,271 \\ \hline\hline
    \multicolumn{3}{|c|}{\textbf{Ising Spin Glass}, N=784, Speedup=20.403} \\ \hline
\textbf{LTGA} & 12,677,619 & 12,601,917 .. 12,761,798 \\ \hline
\textbf{P3} & 621,347 & 568,100 .. 667,236 \\ \hline\hline
    \multicolumn{3}{|c|}{\textbf{MAX-SAT}, N=60, Speedup=942.50} \\ \hline
\textbf{LTGA} & 21,130,960 & 17,576,126 .. 28,314,680 \\ \hline
\textbf{P3} & 22,420 & 14,363 .. 29,161 \\ \hline

  \end{tabular}
	\caption{Comparison of the median evaluations to success for LTGA and P3.  Includes
        	the 95\% bootstrapped confidence interval around the median, as well as
        	the speedup factor for P3 over LTGA.}
	\label{table-results}
\end{table}

When applied to the randomly generated problem classes, P3 appears to outperform LTGA
by an order of complexity.  We suspect some of the cause is LTGA's need to fix
a single population size.  Specifically, during tuning LTGA must set its population
size large enough to solve all of the random instances of that class.  Therefore
all instances which could be solved more efficiently with a smaller population size
are penalized.  P3 on the other hand grows with problem difficulty.  As such its
median evaluation time better reflects the median difficult of a class, whereas LTGA's
evaluation time reflects the maximum difficulty of a class.

Of the runs where LTGA was successful in the median run, it failed to find the optimal solution
109 times, or 0.8\%.  By comparison, P3 failed to find the optimum 3 times.  All three
where on Nearest Neighbor NK, two at size 550 and one at size 600.  Also, P3 was able
to reliably find the optimum on Nearest Neighbor NK $N=650$, $N=700$ and MAX-SAT $N=65$, $N=70$
while LTGA was not.

Table~\ref{table-results} provides a close up look at the largest problem sizes
in which LTGA was able to find the global optimum reliably in less than 100 million
evaluations.  For each, we performed a pairwise Mann-Whitney~U test, all resulting
in p-values less than $10^{-22}$, with the exception of Deceptive Step Trap which
resulted in $0.00025$.  As such these results are highly significant.  The amount
of speedup ranged from 1.1 on Deceptive Step Trap to 942 on MAX-SAT, but in all
cases P3 outperformed LTGA and all other comparison techniques.

As a further verification that our tuning for P3 is valid, we refer
to~\cite{thierens:2013:ltgahiff}, which tested the same variant of LTGA that we
used.  In the section on Child Node filtering, they reported approximately 200,000
mean evaluations to success on the HIFF problem using $N=512$.  Our result of
239,365 is of comparable quality, with P3 solving the same problem using an average
of 92,129 evaluations.  The discrepancy is likely due to the cited work using
their tuning runs in their average, unrealistically allowing each run to use a different tuned population size.

\section{Promising Theory}
Due to P3's persistent integration of random solutions, it always has
a non zero probability of reaching the global optimum.  Because of
this (and unlike LTGA), it is possible to derive expected
running times for P3 on a number of problems.  While we do not present
a rigorous solution to any particular problem here, we wish to
illuminate potential avenues for future proofs.

The traditional One Max class, as well as all functions of unitation, will be
trivially solved by the hill climber in at most $N+1$ evaluations.  These come
from the initial evaluation of the random solution, plus the $N$ evaluations required
to test each gene for improvement.  While more difficult to bound, any unimodal
search space without fitness plateaus will be solved by a single application
of the hill climber.  Notably we expect a bound on Leading Ones of $O(N^2)$ time.
%TODO For future reference, n / factorial(n) + sum(i**2 / factorial(i + 1) for i in range(n))
%is the expected number of improvements made per iteration (converges to 1.718281828459045).

Yet unlike many theory based algorithms, we believe P3 can achieve bounds on more
complex and interesting landscapes.  Consider a sketch for a proof on the Deceptive
Trap problem.  Each restart will require at most $2N$ evaluations to evaluate the
random individual and optimize it to a local optima using the hill climber.  In
$P_0$ each trap in all solutions will either have all 1s or all 0s.  As a result $D(C_i, C_j) = 0$
if all $C_i$ and $C_j$ come from the same trap.  Therefore all crossover clusters
will move only complete traps between solutions, and once there is sufficient
diversity in the population there will be a crossover for each individual trap.
When performing a crossover of a single trap there can only be a single alternative
value, meaning if the candidate solution has all 0s the donor must have all 1s, or vice
versa.  Therefore if $P_i$ contains the optimum solution for a trap in at least one
individual and has a crossover that moves that trap, all solutions produced by crossing
$P_i$ with a candidate solution will have the optimized value of that trap.  Extending to
all traps we can say that once any $P_i$ has at least one solution with the optimized
version of a trap for all traps the very next crossover with $P_i$ will produce the global optimum.
The only remaining portion of the proof is to determine how many evaluations are required to reach this point,
which should be relatively trivial.

Using relaxed requirements, this can likely be extended to any arbitrary fully decomposable
problem. Assuming that for sufficiently large population sizes the entropy between
bits in a single subproblem is lower than the entropy of bits between subproblems,
P3 will construct a cluster capable of moving exactly one subproblem's gene values
during crossover.  As crossover cannot decrease the fitness of a solution each
crossover will have a greater than zero probability of improving subproblem solution quality.
The probability of low fitness local optima for a subproblem reaching high pyramid levels
is therefore low, which increases the probability of donors only having high quality
solutions to subproblems.  Combined this is expected to create a runtime potentially
exponential in subproblem size, but polynomial in problem size.

This proof sketch can likely be extended to the HIFF problem, with minor modifications.
Each application of the hill climber ensures all subproblems of size 2 will be solved.
Once $P_0$ has a sufficient number of solutions it will correctly cross all subproblems
independently, such that all future solutions crossed with $P_0$ will have all subproblems
of size 4 solved.  While $P_1$ can contain solutions that do not correctly solve some 2 bit
subproblems (added before $P_0$ reached critical mass), the probability of these solutions
being selected as donors decreases rapidly (solutions added after $P_0$ reaches critical mass).
As such each level effectively serves to optimize the next largest subproblem size, leading to
convergence using a logarithmic number of levels.


\section{Conclusions and Future Work}
While many parameter-less techniques trade the potential quality of optimal tuning
for general effectiveness, P3 appears to robustly outperform all of the comparison
state of the art techniques across a diverse set of benchmark problems.  This is
because unlike other parameter-less techniques, P3 does not duplicate effort running
multiple populations, nor does it throw away learned information like many evolutionary
techniques.  By striking
a balance between continuously adding diversity and the extreme exploitation provided
by the multiple evaluation crossover, P3 is able to solve easy problems quickly
and hard problems eventually without any problem specific tuning.

Even when compared against LTGA, the closest relative to P3 in current literature
and current state of the art linkage learning algorithm, P3 surpasses expectation.
This is even after LTGA was extensively tuned to each problem class, an unrealistic
proposition on any real world problem.  A likely advantage is that P3 is able to
automatically scale to problem difficulty, adding diversity only as the problem
requires.
%TODO Consider reiterating stuff about gene location invariant, scaling invariant.

P3's ability to efficiently optimize not just single problem instance classes,
but randomly generated problem classes without parameter setting makes it an
ideal candidate for real world application.  Its focus on exploiting all known
information before spending time increasing diversity makes it a great ``stop anytime''
algorithm.  In future work, we plan to investigate in more detail P3's time to
``good enough'' solutions, not just optimal.

While not yet complete, we feel that future work will be able to provide a strong
theoretical foundation for the design of P3.  Here we have presented outlines
for a number of expected runtime proofs, some of which apply to complex problems.
Again, it is P3's mix of strongly elitist selection, paired with continuous integration
of random information that allows for this analysis which cannot be done for many
other state of the art techniques, such as LTGA.

Currently, P3 is limited to single objective, noiseless, binary encoded problems.
In future work we plan to extend (or show no modification is necessary for) P3 to handle
problems from these classes.  Doing so will help make P3 more applicable to a wider
range of real world problems.

Finally, as future work we plan to investigate the individual pieces of P3 for
their usefulness.  While intuitively many of them fit together well, it is possible
that slight modification will allow for even better synergy.  Conversely, the population
pyramid scheme itself may combine well with other optimization techniques.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{../main}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
\balancecolumns
%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
