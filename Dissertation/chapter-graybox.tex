\chapter{New Problem Domain: Gray-Box Optimization}
\label{chap-graybox}
All of the work presented so far has focused on the black-box optimization
domain. These problems are characterized by a complete lack of available
problem specific information. All the algorithm has to perform search
is a ``black-box'' which measures the quality of arbitrary proposed
solutions. Optimizers which are successful in this domain are very general
as there are minimal requirements to apply them to a new problem.

However, for many real world applications there is potentially more information
available. The other end of the spectrum is white box optimization, in which
the algorithm knows the exact problem class it is trying to solve. This domain
is dominated by problem specific search
heuristics~\cite{een:2004:satsolver,sherrington:1975:isingsolver,wright:2000:solvingnk} which
leverage all aspects of the problem to achieve efficiency. This makes such
algorithms very specific, such that outside of their target domain they
either cannot be applied or have no expectation of quality. As such,
each must also be designed by hand for each new problem class, requiring
deep understanding of the problem and the time to develop the algorithm.

In between those two domains there is another: gray-box optimization. In this
domain some features general to multiple problem classes are exploited, beyond
just an evaluation function. The goal is to create optimization methods which
benefits from these features without those methods becoming specialized to
a small set of problem classes.

In addition to the black-box evaluation function, we shall
add in two more exploitable features accessible to search algorithms:
known variable epistasis and partial function evaluation.

\section{Problems in this domain}
To understand the types of problems in this domain, lets first examine
a very simple artificial problem we'll call 3-Equal. The quality
of a solution is determined by how often three consecutive bits are set
to the same value. As a result the maximum fitness is $N$ (all zero or all one) and the minimum fitness
is 0.

The 3-Equal problem has both known epistasis and partial function evaluation. From the definition
we know that each bit is epistatically linked to the two variables that precede it and the two
that follow it. There are no other non-linear relationships with that bit. This also means
it is possible to evaluate the fitness contribution of that bit only knowing the value of at
most four others, regardless of the size of $N$. As we'll discuss in Section~\ref{sec-whitley-ls}
this has enormous implications on search efficiency.

Every problem discussed in Chapter~\ref{chap-problems} except HIFF fit into this domain. Deceptive
Trap and Deceptive Step Trap both can have known epistasis (which bits are in which traps) and
partial reevaluation (score a single trap), the same as Rastrigin (separable). All NK problems
where $K \ll N $ have a knowable epistasis table and the fitness contribution of each subfunction is
calculable without evaluating the entire string. Similarly, MAX-SAT's clause list specifies epistasis,
with each clause independently evaluable. Ising Spin Glasses are even simpler, with each edge in the
graph evaluable using only two problem variables.

Beyond MAX-SAT and Ising Spin Glasses, which are interesting real world problems unto themselves,
many $NP$-Hard real world problems can be expressed in these two requirements. This is especially true
of graph problems.

\begin{itemize}
\item \textbf{Dominating set}: Find a minimum vertex
set such that all vertices are either in the set or adjacent to something in the set.
The fitness of a vertex is determined by if it is in the set (add to set size) and if
it is dominated (add to undominated size). Epistasis is the adjacency set for each
vertex, plus itself. Used in wireless sensor networks~\cite{zhao:2013:dominatingset}.

\item \textbf{Vertex Cover}: Find a minimum vertex
set such that all edges are incident on at least one vertex in the set.
Epistasis and fitness are calculated using each vertex (add to set size)
and each edge (add to uncovered size), independently. Used in network security~\cite{richter:2007:vertexcover}.

\item \textbf{Max-Cut}: Find the set of vertices which maximize the number of edges
incident on exactly one vertex in the set. Epistasis and fitness are calculated
using each edge independently. Used in VLSI design~\cite{festa:2002:maxcut}.

\item \textbf{Set Cover}: Given a universe of elements and a set of subsets of that universe,
find a minimum set of subsets whose union recreates the entire universe. Each element in
the universe is epistatically linked with subsets that contain that element (add to uncovered size)
and each subset also contributes directly to fitness (add to set size). Used in
computational biology~\cite{painsky:2014:setcover}.

\item \textbf{Zero-One Linear Programming}: Given a set of constraints, maximize an
objective function, all of which are linear combinations of variables. Each constraint
creates an epistatic relationship between the variables in that constraint, and fitness
is a linear combination of variables allowing for partial evaluation. Used in power systems~\cite{arroyo:2000:linearprogram}.
\end{itemize}

\section{Formal Requirements}
In order to draw conclusions about search efficiency, it is necessary to make the features
of the target problem domain more explicit.

The overall quality of a solution must be equal to the sum of applying all subfunctions to that
solution. As a consequence each subfunction must be independently evaluable.
The mapping of variables to subfunctions (epistasis) must also be known, and
the maximum number of variables participating in each subfunction is $k$.
To achieve maximum efficiency $k$ should be constant in regards to problem size. The cost
of evaluating a subfunction should also be bounded by some function $b(k)$. For example, on MAX-SAT
$k$ is the clause size, with $b(k)$ linear in clause size, and the fitness equal to the sum of
all clause terms. On NK, $k=K+1$ as each subfunction in NK depends on a variable and $K$ others.

As the mapping of variables to subfunctions is known, it is also possible to calculate $c$, which
is the maximum number of subfunctions any variable participates in. For best efficiency $c$ should
be constant in regards to problem size. For example for all Nearest Neighbor NK landscapes all variables
participate in exactly $c=k=K+1$ subfunctions. Randomly generated MAX-SAT instances have no guarantee
that $c$ is constant, but in general the expected number of subfunctions in which a variable appears is set to 12.81.
Assuming that all subfunctions use $k$ variables and each variable appears in $c$ subfunctions
provides a bound on the total number of subfunctions of $\frac{cN}{k}$.

\section{Efficient Local Search}
\label{sec-whitley-ls}
Section~\ref{sec-hill-climber} presented an efficient method for performing local search for the black
box domain. Each potential bit flip, referred to here as a move, requires the entire solution to be reevaluated, which takes $\Omega(N)$ time.
Furthermore, each time a fitness improving move is made all previously tested moves must be tested again. As a result
improving a random solution to be a local optimum using this algorithm requires between $\Omega(N^2)$ and \BigO{IN^2}, where $I$ is the number
of improving moves. The lower bound is achieved when the while loop in Figure~\ref{fig-hc} runs a constant number of times.
For instance on One Max, Deceptive Trap, Deceptive Step Trap, and HIFF all possible single bit improvements are found during
the first time through the loop. The worst case is when each loop is expected to only make a single improving move, which causes
all $N$ moves to be reevaluated once per improvement, such as on the Leading Ones problem.

Local search in the gray-box domain can be significantly more efficient~\cite{whitley:2013:greedy}. This is due
to two consequences of the domain: the fitness effect of making a move can be calculated in \BigO{1} time, not \BigO{N},
and the number of moves which must be reevaluated per improvement is \BigO{1}, not \BigO{N}. This results in local
search requiring \BigO{N+I} time. If $I$ is within a constant factor of $N$, this means generating random local
optima is at most a constant factor slower than generating random solutions in the search space.

\begin{figure}
  \begin{algorithmic}[1]
  \Procedure{InitializeDelta}{$solution$}
    \State $fitness \leftarrow 0$
    \State $\forall_{m} delta[m] \leftarrow 0$
    \ForAll{$s \in subfunctions$}
      \State $pre\_move \leftarrow f_s(solution)$
      \State $fitness \leftarrow fitness + pre\_move$
      \ForAll{$m \in effected\_moves(s)$}
        \State $post\_move \leftarrow f_s(solution \oplus m)$
        \State $delta[m] \leftarrow delta[m] + (post\_move - pre\_move)$
      \EndFor
    \EndFor
  \EndProcedure
\end{algorithmic}
  \caption{Algorithm used to efficiently determine the change in fitness
           associated with each potential move from a given solution.}
  \label{fig-initialize-delta}
\end{figure}

To achieve this performance, this local search technique begins by determining the change in fitness
caused by making each potential move $m$, denoted as $delta[m]$. Figure~\ref{fig-initialize-delta}
calculates the $delta$ for each $m$ starting at a given solution, as well as the fitness of the solution.
Here, $f_s$ evaluates
the subfunction $s$ on the given solution, and $solution \oplus m$ is the result of making move $m$ on $solution$.
For each of the $\frac{cN}{k}$ subfunctions \Call{InitializeDelta}{} must determine the fitness of that
subfunction before and after making each of $k$ moves which overlap that subfunction. Combined this
results in less than $cN2b(k)$ operations, which is \BigO{N} assuming $c$ and $b(k)$ do not grow with $N$.
Also, this procedure calls $f_s$ only $k+1$ times more than is required to find the fitness of the solution itself.

When performing hill climbing, only moves $m$ such that $delta[m] > 0$ are fitness improving moves. Initially
all moves are considered $open$, meaning that they could potentially be fitness improvements. Each iteration
a random move $m$ is chosen from $open$, and $delta[m]$ is checked. If $m$ is a fitness improving move,
\Call{MakeMove}{$m$}, shown in Figure~\ref{fig-make-move}, is called.

\begin{figure}
  \begin{algorithmic}[1]
  \Procedure{MakeMove}{$m$}
    \State $fitness \leftarrow fitness + delta[m]$
    \ForAll{$s \in effected\_subfunctions(m)$}
      \State $pre\_both \leftarrow f_s(solution)$
      \State $just\_m \leftarrow f_s(solution \oplus m)$
      \ForAll{$m' \in effected\_moves(s)$}
        \State $just\_m' \leftarrow f_s(solution \oplus m')$
        \State $post\_both \leftarrow f_s(solution \oplus m \oplus m')$
        \State $delta[m'] \leftarrow delta[m'] - (just\_m' - pre\_both) + (post\_both - just\_m)$
        \State $open(m')$
      \EndFor
    \EndFor
    \State $close(m)$
    \State $solution \leftarrow solution \oplus m$
  \EndProcedure
\end{algorithmic}
  \caption{Algorithm for updating stored information related to a solution when
           making a move.}
  \label{fig-make-move}
\end{figure}

\Call{MakeMove}{} updates the fitness and $delta$ values to reflect the change in the solution. The fitness
of the solution after making the move does not require any calls to $f_s$ as $delta[m]$ already stores the
change in fitness. This process requires updating all the $delta$ values for the $k$ moves which interact with each of the $c$ subfunctions
effected by $m$. This update replaces outdated information which used the original solution ($just\_m' - pre\_both$)
with how much the move improves over the new solution ($post\_both - just\_m$). As $delta[m']$ has updated, $m'$ could
potentially become a fitness improving move and is therefore added into $open$. As $m$ was just flipped,
it cannot be a fitness improvement and is therefore removed from $open$.  In total this requires less than
$ck4b(k)$ time, which is \BigO{1} assuming $c$, $k$, and $b(k)$ do not scale with $N$.

Each time an improving move is found at most $ck$ additional moves are added to $open$. If $open$ ever
becomes empty a local optimum has been reached. Therefore the number of times $delta[m]$ is checked
is no more than $N + Ick$. As a result the total cost of improving a random solution until it reaches
a global optimum is only \BigO{N+I}.

As an interesting addition, this method can actually be used to efficiently perform approximate steepest ascent hill
climbing~\cite{whitley:2013:greedy}. Instead of choosing moves from $open$ at random, the moves are binned
using their $delta$ values. Moves are then chosen randomly from the highest quality non-empty bin, and changes
in $delta$ can cause moves to change bins. Assuming the total number of bins does not increase with $N$, both steps
can be done in \BigO{1} time. However, at least for MAX-SAT, there is evidence that when used in combination with subsequent
search heuristics the less greedy first improvement algorithm was more effective.

\section{Efficient Hamming Ball Search}
Another consequence of the gray-box domain is that increasing the neighborhood of local search becomes
more tractable~\cite{chicano:2014:ball}.  Instead of improving solutions until no single bit flip
is a fitness improvement, this can be expanded to no flip of $r$ or less bits can improve the solution.

In a black-box setting, verifying that no $r$ bit flip can improve a solution requires testing all
$n \choose r$ neighbors. This quickly becomes intractable as $r$ increases. However in the gray
box domain not all combinations need to be tested. Consider that if two variables do not participate
in the same subfunction, the relationship between their effects is by definition additive. As such
there is no way for flipping both together to be a fitness improvement without flipping one of them
being a fitness improvement. Therefore it is not necessary to try all possible $r$ sized subsets
of the solution.

Consider the 3-Equal problem.  If there are at least two loci between $x_i$ and $x_j$ they do
not share a common subfunction. Therefore, the change in fitness resulting from flipping
both is equal to $delta[x_i] + delta[x_j]$. As such if $delta[x_i] \leq 0$ and $delta[x_j] \leq 0$
then $delta[x_i,x_j] \leq 0$. Now consider a solution such that the first half is set to 0
and the second half is set to 1. Even though $c=3$ there is no way to improve this solution without
simultaneously flipping all 0's to 1's or vice versa. Any smaller flip will not
be a fitness improvement.

To determine which of the $n \choose r$ flips which must be checked, consider a graph where
each vertex is a variable in the solution. An edge exists between two vertices if and only
if the corresponding variables participate in at least one subfunction together. Restated,
there is only an edge if the two variables have a non-linear relationship. The maximum
degree of a vertex in this graph is $c(k-1)$, making it sparse for sufficiently large $N$.
If a subset of vertices is connected then it is possible that flipping all of those
variables together will result in a fitness improvement even when flipping any subset
of the subset will not. However, if the subset is not connected then each component
of the subset can be tested independently.

\begin{figure}
  \begin{algorithmic}[1]
  \Procedure{ConnectedInducedSubgraphs}{}
    \State $closed \leftarrow \emptyset$
    \State $found \leftarrow [~]$
    \ForAll{$v \in V$}
      \State $closed \leftarrow closed \cup \{v\}$
      \State $found \leftarrow found + $\Call{CISG}{$v$, $\emptyset$, $closed$, $\emptyset$}
    \EndFor
    \State \Return $found$
  \EndProcedure
  \Procedure{CISG}{$v$, $subset$, $closed$, $open$}
    \State $subset' \leftarrow subset \cup \{v\}$
    \State $found \leftarrow [subset']$
    \If{$|subset'| \geq r$}
      \Return $found$
    \EndIf
    \State $closed\_here \leftarrow \emptyset$
    \State $open' \leftarrow open \cup adjacent(v)$
    \ForAll{$v' \in open'$ such that $v' \notin closed$}
        \State $closed\_here \leftarrow closed\_here \cup \{v'\}$
        \State $closed \leftarrow closed \cup \{v'\}$
        \State $recurse \leftarrow $\Call{CISG}{$v'$, $subset'$, $closed$, $open'$}
        \State $found \leftarrow found + recurse$
    \EndFor
    \State $closed \leftarrow closed - closed\_here$
    \State \Return $found$
  \EndProcedure
\end{algorithmic}
  \caption{Algorithm to recursively find all connected induced subgraphs of size $r$ or fewer.}
  \label{fig-connected-subgraphs}
\end{figure}

In order to determine which moves in the hamming-ball must be evaluated,
we developed \Call{ConnectedInducedSubgraphs}{} given in Figure~\ref{fig-connected-subgraphs}.
\Call{CISG}{} is a recursive helper function which finds all subgraphs
which contain a given $subset$ and a given vertex $v$, while excluding
any other vertices added to $closed$. To find all subgraphs, \Call{CISG}{}
is called once for each vertex in the graph, such that $closed$ contains
all previously searched vertices, and $subset=open=\emptyset$. In the initial
call all desired subgraphs which contain $v$ are found, which is why $v$ remains
in $closed$ to prevent duplicate subgraphs from being returned.

At each recursive level \Call{CISG}{} expands $open$ to include any vertices
adjacent to $v$ in the graph. By construction this means that $open'$ contains
all possible ways of adding a single vertex to the current $subset'$. As each $v'$
is tested it is temporarily added to $closed$ to prevent recursive calls
from adding it again to $subset'$.

When applied to the sparse graphs inherent in the gray-box domain, this algorithm
requires \BigO{r!(ck)^rN} time, which reduces to \BigO{N}. The time spent
in each call is dominated by the loop over $open'$. In the worst case, $open'$
increases in size by the full adjacency of $v$, which is bounded by $ck$.
This creates a worse case complexity for a single top level call of
$\prod_{i}^{r} ick = r!(ck)^r$. This must be called once for each
of the $N$ variables resulting in \BigO{r!(ck)^rN}.

As this algorithm finds all connected subsets in \BigO{N} time for a fixed $r$, the number of moves
which must be tested to determine if a solution is an $r$-bit local optima is \BigO{N}. This means that
while on Nearest Neighbor NK with $N=8000$, $K=5$, and $r=3$ the black-box method would require 85 billion
checks, the gray-box method requires only 248,000. Even when allowing connections to be completely random,
which results in $c$ increasing with $N$, the gray-box method still only requires approximately 375 million checks,
two and a half orders of magnitude less than assuming a black-box.

From these conclusions it is possible to modify the hill climber presented in Section~\ref{sec-whitley-ls} to
efficiently find $r$-bit local optima~\cite{chicano:2014:ball}. The only change is that instead of having a move
and $delta$ for each bit, there must be a move and delta for each connected induced subgraph of $r$ or less bits.
The efficiency analysis is unchanged with the exception that the constant increases exponentially with $r$ as
the number of $delta$ values which must be updated each move increases. Still, with $c$, $r$, and $k$ constant
with respect to $N$, it is possible to find $r$-bit hamming ball local optima in \BigO{N+I} time.

\section{Tournament Uniform Crossover: TUX}
\label{sec-tux}
\begin{figure}
  \begin{algorithmic}[1]
  \Procedure{Iterate-TUX}{}
    \State Create random $solution$
    \State Hamming-Ball Hill Climb $solution$
    \ForAll{$T_i \in T$}
      \If{$T_i$ is empty}
        \State $T_i \leftarrow solution$
        \State \Return
      \EndIf
      \State Cross $solution$ with $T_i$ to create $2^{i+1}$ offspring
      \State Hamming-Ball Hill Climb each offspring
      \State $solution \leftarrow$ best of offspring, $solution$, and $T_i$
      \State $T_i \leftarrow$ empty
    \EndFor
    \State Add $solution$ to end of $T$
  \EndProcedure
\end{algorithmic}
  \caption{One iteration of TUX optimization. $T$ is an
           ordered list of solutions, each position of which could be empty,
           awaiting a crossover partner.}
  \label{fig-TUX}
\end{figure}

Hamming-Ball Hill Climbing is not sufficient to efficiently find the global optima
on problems with even moderate epistasis~\cite{chicano:2014:ball}. This is because,
like all random restart hill climbers, it relies on random initialization to fall
inside the global optima's basin of attraction.

To remedy this limitation, we set out to develop a minimally complex memetic
algorithm to help increase this probability. Figure~\ref{fig-TUX} presents
the Tournament Uniform Crossover (TUX) algorithm, which combines simplistic
selection with equal probability uniform crossover, the most basic unbiased crossover, to generate starting
solutions likely to be in the global optima's basin of attraction.

Conceptually TUX iteratively builds a structure similar to a single elimination
bracket for solutions. Each ``match'' in the tournament takes in two candidate solutions,
produces offspring via uniform crossover, applies hill climbing to each offspring, with
the ``winner'' being the best of all those solutions. The tournament ``bracket'' is constructed
iteratively, storing an initially empty list of solutions $T$, such that $|T|$ is equal to the height
of the tournament. Iterative construction is possible because when a sub-bracket is complete only a single solution
emerges and solutions only need to be stored until their partner is found.
TUX is fully elitist but does not prematurely converge. This is because search continuously
integrates new randomly generated solutions through other parts of the bracket. Whenever the
top of the current bracket is reached, TUX doubles the size of the virtual tournament.

When crossing solutions at $T_i$, TUX produces $2^{i+1}$ offspring. This relationship
ensures that in total all levels of the tournament, including random initialization,
perform the same number of hill climbing steps. It also shifts the focus of search toward
areas expected to be of higher fitness. The expectation is also that it becomes progressively
harder to improve solutions the higher up the tournament you advance, so more attempts
are necessary to create new useful solutions.

The primary advantages of TUX are that it does not introduce any new parameters (though it still
requires an $r$ for the hill climber) and is relatively simple to implement. Even so
it allows for learning from previous local optima and as Section~\ref{sec-experiments}
will show it is quite effective at optimization.
