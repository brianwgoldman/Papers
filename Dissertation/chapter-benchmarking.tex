\chapter{Benchmarking P3}
\label{chap-benchmarking}

%%%%%%%%%%%%%%%%% Global Optimum %%%%%%%%%%%%%%%%%%%%
\section{Finding the Global Optimum}
\label{sec-optimum}

\begin{figure}
  \begin{center}
  \includegraphicsfit{evals-to-success}
  \end{center}
  \caption{Comparison of the median number of evaluations to reach the global optimum for
           the six different optimization methods with respect
           to problem size.  If the median run did not reach the global optimum no data element
           is shown.  Results given on a log-log scale.}
  \label{fig-evals-to-success}
\end{figure}

Figure~\ref{fig-evals-to-success} shows the median number of evaluations required by each of the six
algorithms to find the global optimum for multiple sizes of each problem. Each data point in
Figure~\ref{fig-evals-to-success} represents the median of 100 runs, where unsuccessful runs
are treated as requiring more evaluations than any successful run. If the median run was not successful
no point is shown.
Medians are used as the data is not normally distributed, and because it allows for more meaningful comparison
between techniques with different success rates.
The maximum problem size used for each problem was set to be the largest size we could
feasibly determine LTGA's optimal population size. For many larger problems results are not shown
for hBOA due to the extreme computational cost required to optimally set the population size.

\subsection{Quantitative Comparison}
Of the 130 tested configurations, P3 found the global optimum using the least median evaluations on 114.
The largest problem size for any problem where P3 was not the most efficient has 49 bits, with P3 achieving the best results on all 92 larger
configurations. hBOA, LTGA, and Parameter-less hBOA only outperform P3 on the smallest 5, 4, and 1 Deceptive
Step Trap instances, respectively. Random Restart Hill Climbing outperforms P3 on the smallest 3 Nearest Neighbor NK instances
and the smallest Ising Spin Glass. $1+(\lambda, \lambda)$ has the most success outperforming P3, doing so
on the smallest 5 Deceptive Traps, 3 smallest Deceptive Step Traps, and 2 smallest Rastrigin.
The likelihood that P3 would achieve these pairwise results assuming its median result is actually worse is $p < 10^{-15}$
according to the binomial test.
Pairwise comparison of LTGA and P3 on the largest problem size using the Mann-Whitney~U test results in
$p < 10^{-5}$ for all problems.

\subsection{Local Search}
The Random Restart Hill Climber and $1+(\lambda, \lambda)$ are both relatively effective on small problem
sizes. This is especially true for the three randomly generated problem classes. These problems
may contain relatively few local optima or just be exceptionally difficult for the model based algorithms.
On Deceptive Trap and Deceptive Step Trap using 4 or less traps, $1+(\lambda, \lambda)$ performs significantly
better than any other algorithm. We believe this is
because $1+(\lambda, \lambda)$ is able to overcome deception by probabilistically
flipping entire traps.
This ability also leads $1+(\lambda, \lambda)$
to outperform the Random Restart Hill Climber on all problems except Nearest Neighbor NK.

On larger problem sizes, the ability for local search to reach the global optimum quickly diminishes.
Only on MAX-SAT are these optimizers competitive
at larger tested problem sizes. However, we believe this is because the largest tested MAX-SAT
was an order of magnitude smaller than the largest size tested for most other problems. As the problem
size increases the number of local optima increases exponentially, which explains why Random Restart Hill
Climbing was unable to scale.
For larger problems it also becomes increasingly unlikely for $1+(\lambda, \lambda)$ to make the right combination
of changes required to reach the global optimum. This
behavior causes high variance in success rate, as evident by the occasional successes on large Deceptive Trap problems.

\subsection{Model Building}
Only techniques which explicitly built models of gene epistasis were able to solve the largest problem
instances. On single instance problems LTGA was more effective than hBOA, with hBOA outperforming
LTGA on Nearest Neighbor NK and Ising Spin Glasses. This may be caused by the differences in modeling method:
unlike the single instance problems, gene epistasis in the randomly generated
problem classes cannot be be perfectly represented with a linkage tree.

Considering how different hBOA and LTGA are in performing optimization, it is somewhat surprising how similar
their results are on HIFF.
However, both techniques rely on populations large enough to support the diversity required to reach the global optimum
and to model epistasis. Both techniques also only rebuild models once per generation. As the subproblems of HIFF
are nested, it is unlikely that either technique can accurately model higher order epistasis before solving
lower order subproblems. Therefore both methods require one generation per subproblem order.

\subsection{P3}
Unlike the other model based methods, P3 generally outperforms
both Random Restart Hill Climber and $1+(\lambda, \lambda)$ even on small problem sizes.
Unlike the other local search methods, P3 outperforms LTGA and hBOA even on large problem sizes.
This implies that P3 is gaining the benefits of each, leveraging local search to solve easy
problems and model building to solve harder ones.

Furthermore, the interaction between these two optimization tools explains some of the reason P3
outperforms each method alone.
On Deceptive Trap P3's use of hill climbing ensures all traps are immediately optimized, allowing for perfect
linkage detection and high quality donation.
On HIFF local search solves all pairwise subproblems, saving P3 a generation over LTGA and hBOA.
In comparison P3 is only a slight improvement on Deceptive Step Trap, which is less amenable to local search.

%%%%%%%%%%%%%%%%%%% Fitness Over Time %%%%%%%%%%%%%%%%%%%%%
\section{Fitness Over Time}
\label{sec-overtime}
\begin{figure}
  \begin{center}
  \includegraphicsfit{fitness-over-time}
  \end{center}
  \caption{Compares the median best fitness reached during search for each of the six optimization methods.}
  \label{fig-fitness-over-time}
\end{figure}

For some applications, finding the global optimum is less important than finding good solutions quickly. Therefore
we examine this behavior in Figure~\ref{fig-fitness-over-time}. At regular intervals during optimization
Figure~\ref{fig-fitness-over-time} shows the median of the best fitnesses found at that point of search across
100 runs. For each problem we show the largest problem size for which we were able to successfully gather results
for all six algorithms, but the trends shown are representative of all larger problem sizes. The maximum reporting interval
is set to include the slowest P3 run to reach the global optimum.

% 181 total
% 121 has P3 best
%   9 hc
%  50 lambdalambda
%  20 hboa
%  18 phboa
%  27 LTGA

\subsection{Quantitative Comparison}
Of 181 sample points, P3 had the highest median fitness in 121. In pairwise competition, $1+(\lambda, \lambda)$
was the most likely to outperform P3, doing so on 50 sample points. LTGA, hBOA, and Parameter-less hBOA
were the next best, outperforming P3 on 27, 20, and 18 sample points, respectively. Random Restart Hill Climbing almost
never outperformed P3, doing so only 9 times.
The likelihood that P3 would achieve these pairwise results assuming its median result is actually worse is $p < 10^{-9}$
according to the binomial test.

\subsection{Local Search}
Perhaps the most striking result is the quality of $1+(\lambda, \lambda)$. Until quite far into search this
method performs better than both LTGA and hBOA. Given sufficient evaluations $1+(\lambda, \lambda)$ also outperforms
Random Restart Hill Climbing on all 7 problems. For brief periods in the middle of search it performs the
best of all techniques on: Deceptive Trap; Deceptive Step Trap; HIFF; Ising Spin Glass; and MAX-SAT problems.
$1+(\lambda, \lambda)$'s ability to efficiently incorporate gene modifications of larger
than one bit allows it to overcome the deception and plateaus in Deceptive Trap and Deceptive Step Trap, solve medium sized subproblems
in HIFF, flip the signs on multiple adjacent bits in Ising Spin Glass, and cross plateaus in MAX-SAT. However,
this method is slow in reaching the global optima in many of these problems which causes it to eventually
be overtaken by the model building techniques.

\subsection{Model Building}
Both hBOA and LTGA are marked by periods of little improvement followed by rapid improvement.
In hBOA this is taken to the extreme, with all fitness
improvement made at the very end of search. In both cases this is caused by model building. Before the model
is accurate little improvement is made. Once it is accurate, fitness improves dramatically.

At 58\% of the recording intervals hBOA has the worst fitness of any solver. Most of the exceptions
occur when hBOA is still evaluating its initial population, allowing this random search to temporarily surpass the
local search methods. After $N$ evaluations, however, hBOA and LTGA both fall behind until their models begin
to improve.
Parameter-less hBOA reaches intermediate fitnesses faster than hBOA, doing so on 62\% of intervals, as its models begin
to optimize earlier than hBOA.  However, this trend is reversed after a sufficient number of evaluations, most clearly
on Deceptive Step Trap and Ising Spin Glasses, as hBOA's tuned population overtakes Parameter-less hBOA's parallel populations.

On every problem LTGA has five distinct periods: fitness plateau, near instantaneous improvement, fitness plateau,
and improvement to global optimum. The early period corresponds with
initialization of the population, with the first fitness gain achieved immediately upon completing the first generation.
When using an inaccurate model, LTGA's mixing strategy performs a sort of less effective local search.
Subsequent generations then make only minor fitness improvements. Once the model
becomes accurate and the probability of a crossover using high quality genetic material increases sufficiently, LTGA
enters a second period of rapid improvement.

\begin{comment}
This behavior is easiest to understand on the two Trap problems. LTGA's first generation will push individual traps toward a
local optima, but it requires more than a single generation to do so completely. Until that occurs, the model is likely behaving
no better than random as there is little apparent linkage between bits. Once the model begins to identify individual traps, crossover
can begin to increase the frequency of higher fitness trap settings. This process begins slowly as low fitness local optima are more
likely to be chosen for donation than high fitness local optima due to their frequency in the population. The process becomes
self-catalysing as increased frequency of the global optimum trap genes means increased likelihood of the global optimum being spread by crossover.
\end{comment}

\subsection{P3}
The integration of hill climbing into P3 makes it strictly better than using hill climbing alone.
Early in optimization P3 and the Random Restart Hill Climber have effectively identical quality. This is because
P3 performs the same evaluations as the Hill Climber for the first two restarts. Once P3 begins
performing crossover it immediately improves over the Hill Climber. In 95\% of intervals P3 had a fitness at
least as high as Hill Climbing. As such P3 is better than a simple
hill climber regardless of how long each technique is run and irrespective of how high quality the solution
found has to be.

Unlike the model based methods, which struggle until model accuracy improves, P3's iterative solution
integration allows it to improve much more quickly. This behavior exists in most problems, but is easiest to understand on Deceptive Trap.
On this problem, P3 immediately brings all traps to local optima, equaled only by the Random Restart Hill Climber in quality.
In comparison LTGA must evaluate the entire population and perform multiple generations to reach similar quality.
P3 is able to immediately integrate optimal versions of each
trap into a single individual as they are found by local search, resulting in smoother fitness improvement than LTGA.

%%%%%%%%%%%%%%%%%%%%%%%%%% Computational Expenses %%%%%%%%%%%%%%%%%%%%
\section{Computational Expenses}
While it is common in evolutionary computation to assume the evaluation function will dominate algorithm
complexity, in some domains this will not be true. Model based methods are especially likely to violate
this norm. Therefore, in order to assess P3's quality in solving problems with efficient fitness functions,
we provide data on both its algorithmic complexity and wall clock time.

\subsection{Operation Counting}
\begin{figure}[t]
  \begin{centering}
    \begin{subfigure}{.5\textwidth}
      \begin{centering}
        \includegraphicsfit{rebuilds}
      \end{centering}
      \caption{Rebuilds}
      \label{fig-rebuilds}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
      \begin{centering}
        \includegraphicsfit{donations}
      \end{centering}
      \caption{Donations}
      \label{fig-donations}
    \end{subfigure}
  \end{centering}
  \caption{Estimated computation costs incurred by model rebuilding (Figure~\ref{fig-rebuilds}) and
           repeated donations (Figure~\ref{fig-donations}) per evaluation as problem size increases.}
  \label{fig-costs}
\end{figure}

When discussing the asymptotic complexity of P3 in Chapter~\ref{chap-p3}, two aspects eluded precise
analysis: how expensive is model rebuilding and how many gene donations are made. Figure~\ref{fig-costs}
provides some insight into how often these two aspects of the algorithm are utilized.

Figure~\ref{fig-rebuilds} reports in an algorithmic sense how expensive model rebuilding is for search.
In order to calculate this value we recorded how many times search rebuilt the model during each run.
Figure~\ref{fig-rebuilds} shows the estimated ratio of model rebuilding cost ($N^2$ per rebuild) over evaluation cost ($N$ per evaluation).
If the cost of model building
scaled linearly with evaluations, the relationship plotted for each problem should be asymptotically constant.
For Nearest Neighbor NK, Ising Spin Glasses, and Rastrigin this is the case. For both Trap problems
and HIFF there is slow growth in the ratio. The problem sizes used for MAX-SAT were not sufficient
to accurately gauge the asymptotic behavior. Together this suggests that while the cost of building the model
is almost linear per evaluation, it can grow slowly. However, even in the worst case (HIFF) this growth was
no more than twice the algorithmic cost of an evaluation even using 2048 bits.

When applying a crossover subset, P3 tries random donors from the population until one is found with at least
one bit different from the improving solution. In theory this can result in up to $O(\mu)$ operations.
Figure~\ref{fig-donations} examines the observed average number of donations per evaluation performed.
Ising Spin Glass, HIFF, and Rastrigin all achieve effectively constant behavior here, implying repeated
donation does not impact the asymptotic runtime of P3. Both Trap functions and Nearest Neighbor NK all
increase in number of donations as problem size increases, potentially increasing algorithmic costs. An
important note is that each donation may range in size from a single bit up to $N-1$. However,
repeated donation attempts are far more likely to happen with smaller clusters.
As such this may cause some super-linear growth
in P3, but its unlikely to be very high.

\subsection{Wall Clock Performance}
\begin{figure}
  \begin{center}
  \includegraphicsfit{seconds-to-success}
  \end{center}
  \caption{Comparison of the median number of seconds to reach the global optimum for
           the six different optimization methods with respect
           to problem size.  If the median run did not reach the global optimum no data element
           is shown.  Results given on a log-log scale.}
  \label{fig-seconds-to-success}
\end{figure}


To assess wall clock performance we provide Figure~\ref{fig-seconds-to-success}.
Similar to Figure~\ref{fig-evals-to-success}, each point represents the median
of 100 runs, with unsuccessful runs treated as slower than successful runs.
\begin{comment}
In total these runs
took over 6 computing years to complete. We were therefore forced to use
diverse hardware potentially running multiple processes in parallel.
As such these timings should be considered very skeptically as a number
of sources may contribute to biased error in the results. However, due to
the size of the dataset and the general regularity in the results some general conclusions
can still be made.
\end{comment}
These results were collected using 2.5GHz Intel Xeon E5-2670v2 processors.

\subsubsection{Model Building}
hBOA and Parameter-less hBOA perform much worse when using wall clock time as the unit of comparison than when using
evaluations. This makes sense as hBOA's model building requires $\Omega(N^2)$ time per evaluation while, under reasonable
assumptions, P3 and LTGA require $O(N)$ time per evaluation. This penalty is most clear on Ising Spin Glass where
hBOA goes from being slightly more efficient than LTGA in terms of evaluations to three orders of magnitude worse in terms of seconds.
As P3 and LTGA require a similar asymptotic complexity per
evaluation as the Hill Climber and $1+(\lambda, \lambda)$, no similar change in ordering occurs.

\subsubsection{P3}
When LTGA is optimally tuned to a single instance problem with an efficient evaluation function
it can find the global optimum faster than P3 in terms of wall clock time. However, on randomly
generated problem classes P3's efficient use of evaluations is enough to overtake LTGA.

On the four single instance problems LTGA not only finds the global optimum using less wall clock time,
the factor speedup increases as problem length does. Na\"{i}vely this suggests LTGA is achieving
a lower order of complexity. However, for these experiments LTGA is growing at sub-linear time per
evaluation, which is not asymptotically stable due to (at minimum) the time required to perform an evaluation.
We suspect that the true cause is that $N$ is small enough to be overshadowed by lower order polynomial terms.
For example, LTGA requires $O(N/\mu)$ time per evaluation to rebuild the linkage
model from the frequency table. As a result, for small $\mu$ model building, and not extracting pairwise
frequency, can dominate runtime.

When applied to randomly generated problem classes, the differences in P3 and LTGA's evaluation complexity
dominates runtime complexity. Similar to with Figure~\ref{fig-evals-to-success}, the amount of speedup P3
achieves over LTGA increases with problem size on Nearest Neighbor NK, Ising Spin Glasses, and MAX-SAT.

Across both types of problems we find that P3's time per evaluation grows approximately linearly. As such,
we conclude that P3 requires asymptotically similar amounts of time per evaluation as the other efficient techniques.

